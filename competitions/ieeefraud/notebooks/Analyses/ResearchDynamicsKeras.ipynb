{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:15.153824Z",
     "start_time": "2019-09-22T19:42:13.517798Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "main_dir = r'../../..'\n",
    "data_path = main_dir+'/data'\n",
    "import sys\n",
    "sys.path.append(main_dir)\n",
    "from typing import List, Set, Dict, Optional, Any, Tuple, Type, Union\n",
    "\n",
    "from BayDS.lib.pipeline import *\n",
    "from BayDS.lib.io import *\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:19.358918Z",
     "start_time": "2019-09-22T19:42:15.156826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "0: LoaderNode [2019-09-22 22:42:15]\n",
      "params:\n",
      " {'input_directory': '../../../Snapshots/1/Test', 'file': 'label_encoded_data.pkl'}\n",
      "---------------------------\n",
      "1: LoaderNode [2019-09-22 22:42:18]\n",
      "params:\n",
      " {'input_directory': '../../../Snapshots/Dynamic', 'file': 'oof_best_submit.csv'}\n",
      "---------------------------\n",
      "2: LoaderNode [2019-09-22 22:42:19]\n",
      "params:\n",
      " {'input_directory': '../../../Snapshots/Dynamic', 'file': 'ieee_with_additions.csv'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = f'{main_dir}/Snapshots/1/Test'\n",
    "p = Pipeline(working_folder=f'{main_dir}/Snapshots/1/DynamicsKeras')\n",
    "\n",
    "p.add_node(LoaderNode, None, 'data',\n",
    "           params={\n",
    "               'input_directory': data_dir,\n",
    "               'file': 'label_encoded_data.pkl'\n",
    "           })\n",
    "p.add_node(LoaderNode, None, 'oof',\n",
    "           params={\n",
    "               'input_directory': main_dir+'/Snapshots/Dynamic',\n",
    "               'file': 'oof_best_submit.csv'\n",
    "           })\n",
    "p.add_node(LoaderNode, None, 'predictions',\n",
    "           params={\n",
    "               'input_directory': main_dir+'/Snapshots/Dynamic',\n",
    "               'file': 'ieee_with_additions.csv'\n",
    "           })\n",
    "p.run(verbose=True)\n",
    "p.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:23.353997Z",
     "start_time": "2019-09-22T19:42:19.364916Z"
    }
   },
   "outputs": [],
   "source": [
    "df = p.data['data']\n",
    "df = df[['isFraud', 'new_card_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:23.552005Z",
     "start_time": "2019-09-22T19:42:23.355999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2987000, 2987001, 2987002, 2987003, 2987004, 2987005, 2987006,\n",
       "            2987007, 2987008, 2987009,\n",
       "            ...\n",
       "            4170230, 4170231, 4170232, 4170233, 4170234, 4170235, 4170236,\n",
       "            4170237, 4170238, 4170239],\n",
       "           dtype='int64', name='TransactionID', length=1097231)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pd.concat([p.data['oof'].set_index('TransactionID')['score'],p.data['predictions'].set_index('TransactionID')['isFraud']],axis=0)#\n",
    "pred.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:23.705006Z",
     "start_time": "2019-09-22T19:42:23.555004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID\n",
       "2987000    0.013343\n",
       "2987001    0.008355\n",
       "2987002    0.003535\n",
       "2987003    0.001327\n",
       "2987004    0.002611\n",
       "             ...   \n",
       "4170235    0.003432\n",
       "4170236    0.002554\n",
       "4170237    0.003562\n",
       "4170238    0.003217\n",
       "4170239    0.002311\n",
       "Length: 1097231, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:23.860009Z",
     "start_time": "2019-09-22T19:42:23.708010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df['pred_0'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:24.006013Z",
     "start_time": "2019-09-22T19:42:23.862009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>new_card_id</th>\n",
       "      <th>pred_0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>130935</td>\n",
       "      <td>0.013343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>47322</td>\n",
       "      <td>0.008355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>40076</td>\n",
       "      <td>0.003535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>9790</td>\n",
       "      <td>0.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>113643</td>\n",
       "      <td>0.002611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170235</td>\n",
       "      <td>-1</td>\n",
       "      <td>21517</td>\n",
       "      <td>0.003432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170236</td>\n",
       "      <td>-1</td>\n",
       "      <td>9161</td>\n",
       "      <td>0.002554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170237</td>\n",
       "      <td>-1</td>\n",
       "      <td>48339</td>\n",
       "      <td>0.003562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170238</td>\n",
       "      <td>-1</td>\n",
       "      <td>132704</td>\n",
       "      <td>0.003217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4170239</td>\n",
       "      <td>-1</td>\n",
       "      <td>87266</td>\n",
       "      <td>0.002311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1097231 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               isFraud  new_card_id    pred_0\n",
       "TransactionID                                \n",
       "2987000              0       130935  0.013343\n",
       "2987001              0        47322  0.008355\n",
       "2987002              0        40076  0.003535\n",
       "2987003              0         9790  0.001327\n",
       "2987004              0       113643  0.002611\n",
       "...                ...          ...       ...\n",
       "4170235             -1        21517  0.003432\n",
       "4170236             -1         9161  0.002554\n",
       "4170237             -1        48339  0.003562\n",
       "4170238             -1       132704  0.003217\n",
       "4170239             -1        87266  0.002311\n",
       "\n",
       "[1097231 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:24.322016Z",
     "start_time": "2019-09-22T19:42:24.010014Z"
    }
   },
   "outputs": [],
   "source": [
    "test = df[['pred_0','new_card_id']].groupby('new_card_id').shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:31.582178Z",
     "start_time": "2019-09-22T19:42:24.324019Z"
    }
   },
   "outputs": [],
   "source": [
    "shifts = {}\n",
    "for shift in range(-20,20):\n",
    "    s = df[['pred_0','new_card_id']].groupby('new_card_id').shift(shift)['pred_0']\n",
    "    shifts[shift] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:31.981214Z",
     "start_time": "2019-09-22T19:42:31.584172Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for k,v in shifts.items():\n",
    "    if k!=0:\n",
    "        df[f'pred_{k}']=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:42:33.428211Z",
     "start_time": "2019-09-22T19:42:31.982185Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[df.isFraud>=0].drop(['isFraud'],axis=1)\n",
    "y = df[df.isFraud>=0]['isFraud']\n",
    "test = df[df.isFraud<0].drop(['isFraud'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:43:38.734808Z",
     "start_time": "2019-09-22T19:43:34.541707Z"
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "encoder = CatBoostEncoder(verbose=1, cols=['new_card_id'])\n",
    "encoder.fit(X, y)\n",
    "\n",
    "X = encoder.transform(X)\n",
    "test = encoder.transform(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:43:43.081853Z",
     "start_time": "2019-09-22T19:43:42.778832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_card_id</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_-20</th>\n",
       "      <th>pred_-19</th>\n",
       "      <th>pred_-18</th>\n",
       "      <th>pred_-17</th>\n",
       "      <th>pred_-16</th>\n",
       "      <th>pred_-15</th>\n",
       "      <th>pred_-14</th>\n",
       "      <th>pred_-13</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2987000</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.013343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987001</td>\n",
       "      <td>0.030910</td>\n",
       "      <td>0.008355</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.456456</td>\n",
       "      <td>0.117105</td>\n",
       "      <td>0.040508</td>\n",
       "      <td>0.024899</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987002</td>\n",
       "      <td>0.015724</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.020098</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987003</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.005135</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.002614</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987004</td>\n",
       "      <td>0.147856</td>\n",
       "      <td>0.002611</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577535</td>\n",
       "      <td>0.034990</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577536</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577537</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>0.003130</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.763504</td>\n",
       "      <td>0.164259</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.003562</td>\n",
       "      <td>0.041620</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>0.013255</td>\n",
       "      <td>0.006538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577538</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.038192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045045</td>\n",
       "      <td>0.037262</td>\n",
       "      <td>0.056185</td>\n",
       "      <td>0.026769</td>\n",
       "      <td>0.096027</td>\n",
       "      <td>0.078420</td>\n",
       "      <td>0.091054</td>\n",
       "      <td>0.125454</td>\n",
       "      <td>0.086342</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577539</td>\n",
       "      <td>0.026642</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.022386</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.119617</td>\n",
       "      <td>0.097617</td>\n",
       "      <td>0.724695</td>\n",
       "      <td>0.014294</td>\n",
       "      <td>0.096930</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.062392</td>\n",
       "      <td>0.002142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590540 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               new_card_id    pred_0  pred_-20  pred_-19  pred_-18  pred_-17  \\\n",
       "TransactionID                                                                  \n",
       "2987000           0.034990  0.013343       NaN       NaN       NaN       NaN   \n",
       "2987001           0.030910  0.008355  0.018132  0.000677  0.003601  0.002565   \n",
       "2987002           0.015724  0.003535  0.000534  0.001498  0.020098  0.007489   \n",
       "2987003           0.000265  0.001327  0.001506  0.001295  0.003040  0.005204   \n",
       "2987004           0.147856  0.002611       NaN       NaN       NaN       NaN   \n",
       "...                    ...       ...       ...       ...       ...       ...   \n",
       "3577535           0.034990  0.002267       NaN       NaN       NaN       NaN   \n",
       "3577536           0.006998  0.004644       NaN       NaN       NaN       NaN   \n",
       "3577537           0.038284  0.003130  0.003385  0.763504  0.164259  0.005626   \n",
       "3577538           0.001750  0.038192       NaN       NaN       NaN       NaN   \n",
       "3577539           0.026642  0.005253  0.004172  0.002821  0.001836  0.002686   \n",
       "\n",
       "               pred_-16  pred_-15  pred_-14  pred_-13  ...   pred_10  \\\n",
       "TransactionID                                          ...             \n",
       "2987000             NaN       NaN       NaN       NaN  ...       NaN   \n",
       "2987001        0.456456  0.117105  0.040508  0.024899  ...       NaN   \n",
       "2987002        0.001951  0.008487  0.003347  0.001273  ...       NaN   \n",
       "2987003        0.005135  0.006308  0.002614  0.001262  ...       NaN   \n",
       "2987004             NaN  0.016008  0.000480  0.000690  ...       NaN   \n",
       "...                 ...       ...       ...       ...  ...       ...   \n",
       "3577535             NaN       NaN       NaN       NaN  ...       NaN   \n",
       "3577536             NaN       NaN       NaN       NaN  ...       NaN   \n",
       "3577537        0.000096  0.002307  0.005009  0.004402  ...  0.006830   \n",
       "3577538             NaN       NaN       NaN       NaN  ...  0.045045   \n",
       "3577539        0.022386  0.001228  0.002083  0.011707  ...  0.002546   \n",
       "\n",
       "                pred_11   pred_12   pred_13   pred_14   pred_15   pred_16  \\\n",
       "TransactionID                                                               \n",
       "2987000             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2987001             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2987002             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2987003             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2987004             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "3577535             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3577536             NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3577537        0.003486  0.004347  0.005189  0.008216  0.003562  0.041620   \n",
       "3577538        0.037262  0.056185  0.026769  0.096027  0.078420  0.091054   \n",
       "3577539        0.001088  0.119617  0.097617  0.724695  0.014294  0.096930   \n",
       "\n",
       "                pred_17   pred_18   pred_19  \n",
       "TransactionID                                \n",
       "2987000             NaN       NaN       NaN  \n",
       "2987001             NaN       NaN       NaN  \n",
       "2987002             NaN       NaN       NaN  \n",
       "2987003             NaN       NaN       NaN  \n",
       "2987004             NaN       NaN       NaN  \n",
       "...                 ...       ...       ...  \n",
       "3577535             NaN       NaN       NaN  \n",
       "3577536             NaN       NaN       NaN  \n",
       "3577537        0.032738  0.013255  0.006538  \n",
       "3577538        0.125454  0.086342       NaN  \n",
       "3577539        0.003028  0.062392  0.002142  \n",
       "\n",
       "[590540 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:43:54.177235Z",
     "start_time": "2019-09-22T19:43:54.052232Z"
    }
   },
   "outputs": [],
   "source": [
    "#Keras   strart\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:44:09.818616Z",
     "start_time": "2019-09-22T19:43:56.548336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_card_id\n",
      "pred_0\n",
      "pred_-20\n",
      "pred_-19\n",
      "pred_-18\n",
      "pred_-17\n",
      "pred_-16\n",
      "pred_-15\n",
      "pred_-14\n",
      "pred_-13\n",
      "pred_-12\n",
      "pred_-11\n",
      "pred_-10\n",
      "pred_-9\n",
      "pred_-8\n",
      "pred_-7\n",
      "pred_-6\n",
      "pred_-5\n",
      "pred_-4\n",
      "pred_-3\n",
      "pred_-2\n",
      "pred_-1\n",
      "pred_1\n",
      "pred_2\n",
      "pred_3\n",
      "pred_4\n",
      "pred_5\n",
      "pred_6\n",
      "pred_7\n",
      "pred_8\n",
      "pred_9\n",
      "pred_10\n",
      "pred_11\n",
      "pred_12\n",
      "pred_13\n",
      "pred_14\n",
      "pred_15\n",
      "pred_16\n",
      "pred_17\n",
      "pred_18\n",
      "pred_19\n"
     ]
    }
   ],
   "source": [
    "for column in X.columns:\n",
    "    print(column)\n",
    "    col = pd.concat([X[column],test[column]])\n",
    "    filtered = col[~col.isin([np.inf, -np.inf])].dropna()\n",
    "    vmax = filtered.max()\n",
    "    vmin = filtered.min()\n",
    "    vmean = filtered.mean()\n",
    "    col = col.replace({\n",
    "        np.inf:vmax,\n",
    "        -np.inf:vmin\n",
    "       }).fillna(-1)\n",
    "    if vmax > 100 and vmin >= 0:\n",
    "        col = np.log1p(col)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    col.values[...] = scaler.fit_transform(col.values.reshape(-1,1)).flatten()\n",
    "    \n",
    "    X[column] = col[X.index]\n",
    "    test[column] = col[test.index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:44:12.506199Z",
     "start_time": "2019-09-22T19:44:12.216158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_card_id</th>\n",
       "      <th>pred_0</th>\n",
       "      <th>pred_-20</th>\n",
       "      <th>pred_-19</th>\n",
       "      <th>pred_-18</th>\n",
       "      <th>pred_-17</th>\n",
       "      <th>pred_-16</th>\n",
       "      <th>pred_-15</th>\n",
       "      <th>pred_-14</th>\n",
       "      <th>pred_-13</th>\n",
       "      <th>...</th>\n",
       "      <th>pred_10</th>\n",
       "      <th>pred_11</th>\n",
       "      <th>pred_12</th>\n",
       "      <th>pred_13</th>\n",
       "      <th>pred_14</th>\n",
       "      <th>pred_15</th>\n",
       "      <th>pred_16</th>\n",
       "      <th>pred_17</th>\n",
       "      <th>pred_18</th>\n",
       "      <th>pred_19</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TransactionID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2987000</td>\n",
       "      <td>-0.004654</td>\n",
       "      <td>-0.097789</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>-0.865575</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.890946</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>-0.921882</td>\n",
       "      <td>-0.940055</td>\n",
       "      <td>-0.960388</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987001</td>\n",
       "      <td>-0.052361</td>\n",
       "      <td>-0.141396</td>\n",
       "      <td>1.103609</td>\n",
       "      <td>1.055995</td>\n",
       "      <td>1.046629</td>\n",
       "      <td>1.028610</td>\n",
       "      <td>1.879336</td>\n",
       "      <td>1.211807</td>\n",
       "      <td>1.045562</td>\n",
       "      <td>0.994377</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987002</td>\n",
       "      <td>-0.229947</td>\n",
       "      <td>-0.183534</td>\n",
       "      <td>1.069764</td>\n",
       "      <td>1.057572</td>\n",
       "      <td>1.078260</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.010259</td>\n",
       "      <td>1.004345</td>\n",
       "      <td>0.974647</td>\n",
       "      <td>0.949317</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987003</td>\n",
       "      <td>-0.410723</td>\n",
       "      <td>-0.202837</td>\n",
       "      <td>1.071634</td>\n",
       "      <td>1.057181</td>\n",
       "      <td>1.045553</td>\n",
       "      <td>1.033662</td>\n",
       "      <td>1.016348</td>\n",
       "      <td>1.000182</td>\n",
       "      <td>0.973249</td>\n",
       "      <td>0.949296</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2987004</td>\n",
       "      <td>1.315183</td>\n",
       "      <td>-0.191614</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>-0.865575</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.890946</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>1.018710</td>\n",
       "      <td>0.969177</td>\n",
       "      <td>0.948204</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577535</td>\n",
       "      <td>-0.004654</td>\n",
       "      <td>-0.194622</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>-0.865575</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.890946</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>-0.921882</td>\n",
       "      <td>-0.940055</td>\n",
       "      <td>-0.960388</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577536</td>\n",
       "      <td>-0.331989</td>\n",
       "      <td>-0.173845</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>-0.865575</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.890946</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>-0.921882</td>\n",
       "      <td>-0.940055</td>\n",
       "      <td>-0.960388</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041028</td>\n",
       "      <td>-1.010161</td>\n",
       "      <td>-0.983543</td>\n",
       "      <td>-0.960376</td>\n",
       "      <td>-0.940032</td>\n",
       "      <td>-0.921857</td>\n",
       "      <td>-0.905595</td>\n",
       "      <td>-0.890938</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577537</td>\n",
       "      <td>0.033864</td>\n",
       "      <td>-0.187079</td>\n",
       "      <td>1.075247</td>\n",
       "      <td>2.520829</td>\n",
       "      <td>1.354671</td>\n",
       "      <td>1.034470</td>\n",
       "      <td>1.006714</td>\n",
       "      <td>0.992541</td>\n",
       "      <td>0.977819</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881484</td>\n",
       "      <td>0.903221</td>\n",
       "      <td>0.930403</td>\n",
       "      <td>0.955353</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.993447</td>\n",
       "      <td>1.084601</td>\n",
       "      <td>1.084904</td>\n",
       "      <td>1.063724</td>\n",
       "      <td>1.065855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577538</td>\n",
       "      <td>-0.393364</td>\n",
       "      <td>0.119448</td>\n",
       "      <td>-0.854498</td>\n",
       "      <td>-0.865575</td>\n",
       "      <td>-0.877646</td>\n",
       "      <td>-0.890946</td>\n",
       "      <td>-0.905609</td>\n",
       "      <td>-0.921882</td>\n",
       "      <td>-0.940055</td>\n",
       "      <td>-0.960388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954454</td>\n",
       "      <td>0.967624</td>\n",
       "      <td>1.029190</td>\n",
       "      <td>0.996480</td>\n",
       "      <td>1.149898</td>\n",
       "      <td>1.136313</td>\n",
       "      <td>1.179054</td>\n",
       "      <td>1.262289</td>\n",
       "      <td>1.203755</td>\n",
       "      <td>-0.865579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3577539</td>\n",
       "      <td>-0.102272</td>\n",
       "      <td>-0.168514</td>\n",
       "      <td>1.076762</td>\n",
       "      <td>1.060113</td>\n",
       "      <td>1.043245</td>\n",
       "      <td>1.028841</td>\n",
       "      <td>1.049334</td>\n",
       "      <td>0.990480</td>\n",
       "      <td>0.972236</td>\n",
       "      <td>0.969217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873303</td>\n",
       "      <td>0.898649</td>\n",
       "      <td>1.150068</td>\n",
       "      <td>1.131505</td>\n",
       "      <td>2.348655</td>\n",
       "      <td>1.013928</td>\n",
       "      <td>1.190280</td>\n",
       "      <td>1.028062</td>\n",
       "      <td>1.157869</td>\n",
       "      <td>1.057418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590540 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               new_card_id    pred_0  pred_-20  pred_-19  pred_-18  pred_-17  \\\n",
       "TransactionID                                                                  \n",
       "2987000          -0.004654 -0.097789 -0.854498 -0.865575 -0.877646 -0.890946   \n",
       "2987001          -0.052361 -0.141396  1.103609  1.055995  1.046629  1.028610   \n",
       "2987002          -0.229947 -0.183534  1.069764  1.057572  1.078260  1.038037   \n",
       "2987003          -0.410723 -0.202837  1.071634  1.057181  1.045553  1.033662   \n",
       "2987004           1.315183 -0.191614 -0.854498 -0.865575 -0.877646 -0.890946   \n",
       "...                    ...       ...       ...       ...       ...       ...   \n",
       "3577535          -0.004654 -0.194622 -0.854498 -0.865575 -0.877646 -0.890946   \n",
       "3577536          -0.331989 -0.173845 -0.854498 -0.865575 -0.877646 -0.890946   \n",
       "3577537           0.033864 -0.187079  1.075247  2.520829  1.354671  1.034470   \n",
       "3577538          -0.393364  0.119448 -0.854498 -0.865575 -0.877646 -0.890946   \n",
       "3577539          -0.102272 -0.168514  1.076762  1.060113  1.043245  1.028841   \n",
       "\n",
       "               pred_-16  pred_-15  pred_-14  pred_-13  ...   pred_10  \\\n",
       "TransactionID                                          ...             \n",
       "2987000       -0.905609 -0.921882 -0.940055 -0.960388  ... -1.041028   \n",
       "2987001        1.879336  1.211807  1.045562  0.994377  ... -1.041028   \n",
       "2987002        1.010259  1.004345  0.974647  0.949317  ... -1.041028   \n",
       "2987003        1.016348  1.000182  0.973249  0.949296  ... -1.041028   \n",
       "2987004       -0.905609  1.018710  0.969177  0.948204  ... -1.041028   \n",
       "...                 ...       ...       ...       ...  ...       ...   \n",
       "3577535       -0.905609 -0.921882 -0.940055 -0.960388  ... -1.041028   \n",
       "3577536       -0.905609 -0.921882 -0.940055 -0.960388  ... -1.041028   \n",
       "3577537        1.006714  0.992541  0.977819  0.955284  ...  0.881484   \n",
       "3577538       -0.905609 -0.921882 -0.940055 -0.960388  ...  0.954454   \n",
       "3577539        1.049334  0.990480  0.972236  0.969217  ...  0.873303   \n",
       "\n",
       "                pred_11   pred_12   pred_13   pred_14   pred_15   pred_16  \\\n",
       "TransactionID                                                               \n",
       "2987000       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "2987001       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "2987002       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "2987003       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "2987004       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "...                 ...       ...       ...       ...       ...       ...   \n",
       "3577535       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "3577536       -1.010161 -0.983543 -0.960376 -0.940032 -0.921857 -0.905595   \n",
       "3577537        0.903221  0.930403  0.955353  0.982456  0.993447  1.084601   \n",
       "3577538        0.967624  1.029190  0.996480  1.149898  1.136313  1.179054   \n",
       "3577539        0.898649  1.150068  1.131505  2.348655  1.013928  1.190280   \n",
       "\n",
       "                pred_17   pred_18   pred_19  \n",
       "TransactionID                                \n",
       "2987000       -0.890938 -0.877646 -0.865579  \n",
       "2987001       -0.890938 -0.877646 -0.865579  \n",
       "2987002       -0.890938 -0.877646 -0.865579  \n",
       "2987003       -0.890938 -0.877646 -0.865579  \n",
       "2987004       -0.890938 -0.877646 -0.865579  \n",
       "...                 ...       ...       ...  \n",
       "3577535       -0.890938 -0.877646 -0.865579  \n",
       "3577536       -0.890938 -0.877646 -0.865579  \n",
       "3577537        1.084904  1.063724  1.065855  \n",
       "3577538        1.262289  1.203755 -0.865579  \n",
       "3577539        1.028062  1.157869  1.057418  \n",
       "\n",
       "[590540 rows x 41 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:44:16.825361Z",
     "start_time": "2019-09-22T19:44:16.664369Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "n_fold = 5\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "folds = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:44:22.046468Z",
     "start_time": "2019-09-22T19:44:18.631420Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from BayDS.lib.training import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T19:45:03.862125Z",
     "start_time": "2019-09-22T19:45:03.599119Z"
    }
   },
   "outputs": [],
   "source": [
    "def NNModel_maker():\n",
    "    k.clear_session()\n",
    "    \n",
    "#     categorical_inputs = []\n",
    "#     for cat in categorical:\n",
    "#         categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "\n",
    "#     categorical_embeddings = []\n",
    "#     for i, cat in enumerate(categorical):\n",
    "#         categorical_embeddings.append(\n",
    "#             Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \\\n",
    "#                       \"_embed\")(categorical_inputs[i]))\n",
    "\n",
    "#     categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n",
    "# \n",
    "    numerical_inputs = Input(shape=[X.shape[1]], name = 'all')\n",
    "    numerical_logits = Dropout(.3)(numerical_inputs)\n",
    "  \n",
    "    x = numerical_logits\n",
    "\n",
    "    x = Dense(150, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(70, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    out = Dense(1, activation = 'sigmoid')(x)    \n",
    "\n",
    "    model = Model(inputs= [numerical_inputs],outputs=out)\n",
    "    loss = \"binary_crossentropy\"\n",
    "    model.compile(optimizer=Adam(lr = 0.0003), loss = loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': 8000,\n",
    "    'epochs': 130,\n",
    "    'verbose': True,\n",
    "         }\n",
    "train_options = {\n",
    "    \"model_type\":'keras',\n",
    "    \"params\": params,\n",
    "    \"eval_metric\":'auc',\n",
    "    'averaging': 'usual',\n",
    "    'use_groups': False,\n",
    "    'fold_name': folds.__class__.__name__,\n",
    "    'n_splits': n_fold\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T20:03:07.398211Z",
     "start_time": "2019-09-22T19:45:06.024192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Sep 22 22:45:06 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.2625 - val_loss: 0.0996\n",
      "Epoch 2/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1365 - val_loss: 0.0598\n",
      "Epoch 3/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1120 - val_loss: 0.0524\n",
      "Epoch 4/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1067 - val_loss: 0.0516\n",
      "Epoch 5/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.1026 - val_loss: 0.0517\n",
      "Epoch 6/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0993 - val_loss: 0.0520\n",
      "Epoch 7/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0975 - val_loss: 0.0528\n",
      "Epoch 8/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0960 - val_loss: 0.0536\n",
      "Epoch 9/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0943 - val_loss: 0.0538\n",
      "Epoch 10/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0926 - val_loss: 0.0554\n",
      "Epoch 11/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0918 - val_loss: 0.0552\n",
      "Epoch 12/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0905 - val_loss: 0.0562\n",
      "Epoch 13/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0908 - val_loss: 0.0561\n",
      "Epoch 14/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0902 - val_loss: 0.0570\n",
      "Epoch 15/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0883 - val_loss: 0.0574\n",
      "Epoch 16/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0890 - val_loss: 0.0578\n",
      "Epoch 17/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0881 - val_loss: 0.0578\n",
      "Epoch 18/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0876 - val_loss: 0.0598\n",
      "Epoch 19/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0873 - val_loss: 0.0587\n",
      "Epoch 20/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0870 - val_loss: 0.0596\n",
      "Epoch 21/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0865 - val_loss: 0.0602\n",
      "Epoch 22/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0865 - val_loss: 0.0611\n",
      "Epoch 23/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0854 - val_loss: 0.0603\n",
      "Epoch 24/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0844 - val_loss: 0.0601\n",
      "Epoch 25/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0843 - val_loss: 0.0591\n",
      "Epoch 26/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0837 - val_loss: 0.0593\n",
      "Epoch 27/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0832 - val_loss: 0.0599\n",
      "Epoch 28/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0828 - val_loss: 0.0581\n",
      "Epoch 29/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0839 - val_loss: 0.0597\n",
      "Epoch 30/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0831 - val_loss: 0.0601\n",
      "Epoch 31/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0814 - val_loss: 0.0590\n",
      "Epoch 32/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0823 - val_loss: 0.0577\n",
      "Epoch 33/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0826 - val_loss: 0.0584\n",
      "Epoch 34/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0821 - val_loss: 0.0607\n",
      "Epoch 35/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0815 - val_loss: 0.0582\n",
      "Epoch 36/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0815 - val_loss: 0.0589\n",
      "Epoch 37/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0812 - val_loss: 0.0582\n",
      "Epoch 38/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0807 - val_loss: 0.0586\n",
      "Epoch 39/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0807 - val_loss: 0.0569\n",
      "Epoch 40/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0810 - val_loss: 0.0576\n",
      "Epoch 41/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0806 - val_loss: 0.0572\n",
      "Epoch 42/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0798 - val_loss: 0.0567\n",
      "Epoch 43/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0797 - val_loss: 0.0565\n",
      "Epoch 44/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0792 - val_loss: 0.0557\n",
      "Epoch 45/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0801 - val_loss: 0.0580\n",
      "Epoch 46/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0795 - val_loss: 0.0563\n",
      "Epoch 47/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0797 - val_loss: 0.0550\n",
      "Epoch 48/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0789 - val_loss: 0.0559\n",
      "Epoch 49/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0792 - val_loss: 0.0556\n",
      "Epoch 50/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0796 - val_loss: 0.0554\n",
      "Epoch 51/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0788 - val_loss: 0.0550\n",
      "Epoch 52/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0788 - val_loss: 0.0552\n",
      "Epoch 53/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0782 - val_loss: 0.0538\n",
      "Epoch 54/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0787 - val_loss: 0.0543\n",
      "Epoch 55/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0778 - val_loss: 0.0535\n",
      "Epoch 56/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0785 - val_loss: 0.0538\n",
      "Epoch 57/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0786 - val_loss: 0.0544\n",
      "Epoch 58/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0782 - val_loss: 0.0539\n",
      "Epoch 59/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0782 - val_loss: 0.0531\n",
      "Epoch 60/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0778 - val_loss: 0.0530\n",
      "Epoch 61/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0771 - val_loss: 0.0535\n",
      "Epoch 62/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0774 - val_loss: 0.0534\n",
      "Epoch 63/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0772 - val_loss: 0.0527\n",
      "Epoch 64/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0770 - val_loss: 0.0531\n",
      "Epoch 65/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0766 - val_loss: 0.0532\n",
      "Epoch 66/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0774 - val_loss: 0.0517\n",
      "Epoch 67/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0767 - val_loss: 0.0523\n",
      "Epoch 68/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0765 - val_loss: 0.0517\n",
      "Epoch 69/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0769 - val_loss: 0.0519\n",
      "Epoch 70/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0770 - val_loss: 0.0512\n",
      "Epoch 71/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0772 - val_loss: 0.0516\n",
      "Epoch 72/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0772 - val_loss: 0.0507\n",
      "Epoch 73/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0763 - val_loss: 0.0512\n",
      "Epoch 74/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0762 - val_loss: 0.0512\n",
      "Epoch 75/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0764 - val_loss: 0.0508\n",
      "Epoch 76/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0756 - val_loss: 0.0497\n",
      "Epoch 77/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0762 - val_loss: 0.0504\n",
      "Epoch 78/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0759 - val_loss: 0.0501\n",
      "Epoch 79/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0754 - val_loss: 0.0494\n",
      "Epoch 80/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0763 - val_loss: 0.0501\n",
      "Epoch 81/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0758 - val_loss: 0.0488\n",
      "Epoch 82/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0758 - val_loss: 0.0495\n",
      "Epoch 83/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0757 - val_loss: 0.0492\n",
      "Epoch 84/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0753 - val_loss: 0.0504\n",
      "Epoch 85/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0758 - val_loss: 0.0494\n",
      "Epoch 86/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0754 - val_loss: 0.0494\n",
      "Epoch 87/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0753 - val_loss: 0.0491\n",
      "Epoch 88/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0750 - val_loss: 0.0487\n",
      "Epoch 89/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0753 - val_loss: 0.0490\n",
      "Epoch 90/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0481\n",
      "Epoch 91/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0745 - val_loss: 0.0482\n",
      "Epoch 92/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0484\n",
      "Epoch 93/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0747 - val_loss: 0.0480\n",
      "Epoch 94/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0750 - val_loss: 0.0480\n",
      "Epoch 95/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0479\n",
      "Epoch 96/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0748 - val_loss: 0.0479\n",
      "Epoch 97/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0477\n",
      "Epoch 98/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0743 - val_loss: 0.0479\n",
      "Epoch 99/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0738 - val_loss: 0.0477\n",
      "Epoch 100/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0480\n",
      "Epoch 101/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0469\n",
      "Epoch 102/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0739 - val_loss: 0.0470\n",
      "Epoch 103/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0477\n",
      "Epoch 104/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0740 - val_loss: 0.0468\n",
      "Epoch 105/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0471\n",
      "Epoch 106/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0737 - val_loss: 0.0475\n",
      "Epoch 107/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0745 - val_loss: 0.0467\n",
      "Epoch 108/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0734 - val_loss: 0.0466\n",
      "Epoch 109/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0740 - val_loss: 0.0473\n",
      "Epoch 110/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0737 - val_loss: 0.0464\n",
      "Epoch 111/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0733 - val_loss: 0.0465\n",
      "Epoch 112/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0737 - val_loss: 0.0461\n",
      "Epoch 113/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0465\n",
      "Epoch 114/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0735 - val_loss: 0.0461\n",
      "Epoch 115/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0729 - val_loss: 0.0461\n",
      "Epoch 116/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0454\n",
      "Epoch 117/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0737 - val_loss: 0.0464\n",
      "Epoch 118/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0730 - val_loss: 0.0463\n",
      "Epoch 119/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0734 - val_loss: 0.0467\n",
      "Epoch 120/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0736 - val_loss: 0.0466\n",
      "Epoch 121/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0734 - val_loss: 0.0457\n",
      "Epoch 122/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0460\n",
      "Epoch 123/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0726 - val_loss: 0.0453\n",
      "Epoch 124/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0735 - val_loss: 0.0458\n",
      "Epoch 125/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0728 - val_loss: 0.0453\n",
      "Epoch 126/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0728 - val_loss: 0.0462\n",
      "Epoch 127/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0732 - val_loss: 0.0457\n",
      "Epoch 128/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0732 - val_loss: 0.0459\n",
      "Epoch 129/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0729 - val_loss: 0.0463\n",
      "Epoch 130/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0454\n",
      "118108/118108 [==============================] - 0s 1us/step\n",
      "Fold 0. auc: 0.9665.\n",
      "Fold 2 started at Sun Sep 22 22:48:47 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.2454 - val_loss: 0.1297\n",
      "Epoch 2/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1249 - val_loss: 0.0771\n",
      "Epoch 3/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.1049 - val_loss: 0.0678\n",
      "Epoch 4/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0990 - val_loss: 0.0654\n",
      "Epoch 5/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0960 - val_loss: 0.0640\n",
      "Epoch 6/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0931 - val_loss: 0.0633\n",
      "Epoch 7/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0930 - val_loss: 0.0631\n",
      "Epoch 8/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0921 - val_loss: 0.0632\n",
      "Epoch 9/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0892 - val_loss: 0.0632\n",
      "Epoch 10/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0882 - val_loss: 0.0634\n",
      "Epoch 11/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0875 - val_loss: 0.0635\n",
      "Epoch 12/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0867 - val_loss: 0.0637\n",
      "Epoch 13/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0858 - val_loss: 0.0637\n",
      "Epoch 14/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0850 - val_loss: 0.0637\n",
      "Epoch 15/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0843 - val_loss: 0.0639\n",
      "Epoch 16/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0832 - val_loss: 0.0636\n",
      "Epoch 17/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0842 - val_loss: 0.0642\n",
      "Epoch 18/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0829 - val_loss: 0.0637\n",
      "Epoch 19/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0829 - val_loss: 0.0640\n",
      "Epoch 20/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0820 - val_loss: 0.0636\n",
      "Epoch 21/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0818 - val_loss: 0.0634\n",
      "Epoch 22/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0810 - val_loss: 0.0637\n",
      "Epoch 23/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0810 - val_loss: 0.0634\n",
      "Epoch 24/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0806 - val_loss: 0.0629\n",
      "Epoch 25/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0800 - val_loss: 0.0628\n",
      "Epoch 26/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0801 - val_loss: 0.0627\n",
      "Epoch 27/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0794 - val_loss: 0.0626\n",
      "Epoch 28/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0795 - val_loss: 0.0622\n",
      "Epoch 29/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0784 - val_loss: 0.0622\n",
      "Epoch 30/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0791 - val_loss: 0.0621\n",
      "Epoch 31/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0784 - val_loss: 0.0621\n",
      "Epoch 32/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0780 - val_loss: 0.0616\n",
      "Epoch 33/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0771 - val_loss: 0.0614\n",
      "Epoch 34/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0774 - val_loss: 0.0611\n",
      "Epoch 35/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0780 - val_loss: 0.0613\n",
      "Epoch 36/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0771 - val_loss: 0.0610\n",
      "Epoch 37/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0767 - val_loss: 0.0605\n",
      "Epoch 38/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0760 - val_loss: 0.0606\n",
      "Epoch 39/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0769 - val_loss: 0.0605\n",
      "Epoch 40/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0762 - val_loss: 0.0606\n",
      "Epoch 41/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0758 - val_loss: 0.0606\n",
      "Epoch 42/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0751 - val_loss: 0.0596\n",
      "Epoch 43/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0755 - val_loss: 0.0601\n",
      "Epoch 44/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0751 - val_loss: 0.0599\n",
      "Epoch 45/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0752 - val_loss: 0.0596\n",
      "Epoch 46/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0750 - val_loss: 0.0598\n",
      "Epoch 47/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0750 - val_loss: 0.0596\n",
      "Epoch 48/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0745 - val_loss: 0.0595\n",
      "Epoch 49/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0591\n",
      "Epoch 50/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0589\n",
      "Epoch 51/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0746 - val_loss: 0.0591\n",
      "Epoch 52/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0749 - val_loss: 0.0590\n",
      "Epoch 53/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0741 - val_loss: 0.0589\n",
      "Epoch 54/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0742 - val_loss: 0.0589\n",
      "Epoch 55/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0737 - val_loss: 0.0591\n",
      "Epoch 56/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0737 - val_loss: 0.0590\n",
      "Epoch 57/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0737 - val_loss: 0.0588\n",
      "Epoch 58/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0733 - val_loss: 0.0589\n",
      "Epoch 59/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0732 - val_loss: 0.0590\n",
      "Epoch 60/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0734 - val_loss: 0.0584\n",
      "Epoch 61/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0725 - val_loss: 0.0581\n",
      "Epoch 62/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0736 - val_loss: 0.0588\n",
      "Epoch 63/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0735 - val_loss: 0.0585\n",
      "Epoch 64/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0727 - val_loss: 0.0586\n",
      "Epoch 65/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0734 - val_loss: 0.0584\n",
      "Epoch 66/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0725 - val_loss: 0.0583\n",
      "Epoch 67/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0727 - val_loss: 0.0583\n",
      "Epoch 68/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0719 - val_loss: 0.0581\n",
      "Epoch 69/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0582\n",
      "Epoch 70/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0720 - val_loss: 0.0582\n",
      "Epoch 71/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0722 - val_loss: 0.0582\n",
      "Epoch 72/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0723 - val_loss: 0.0583\n",
      "Epoch 73/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0717 - val_loss: 0.0581\n",
      "Epoch 74/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0714 - val_loss: 0.0581\n",
      "Epoch 75/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0715 - val_loss: 0.0583\n",
      "Epoch 76/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0715 - val_loss: 0.0578\n",
      "Epoch 77/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0716 - val_loss: 0.0579\n",
      "Epoch 78/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0709 - val_loss: 0.0580\n",
      "Epoch 79/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0709 - val_loss: 0.0577\n",
      "Epoch 80/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0713 - val_loss: 0.0580\n",
      "Epoch 81/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0711 - val_loss: 0.0577\n",
      "Epoch 82/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0717 - val_loss: 0.0579\n",
      "Epoch 83/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0708 - val_loss: 0.0579\n",
      "Epoch 84/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0710 - val_loss: 0.0581\n",
      "Epoch 85/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0703 - val_loss: 0.0579\n",
      "Epoch 86/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0708 - val_loss: 0.0579\n",
      "Epoch 87/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0708 - val_loss: 0.0581\n",
      "Epoch 88/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0709 - val_loss: 0.0580\n",
      "Epoch 89/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0705 - val_loss: 0.0581\n",
      "Epoch 90/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0708 - val_loss: 0.0584\n",
      "Epoch 91/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0710 - val_loss: 0.0582\n",
      "Epoch 92/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0704 - val_loss: 0.0582\n",
      "Epoch 93/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0704 - val_loss: 0.0582\n",
      "Epoch 94/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0700 - val_loss: 0.0581\n",
      "Epoch 95/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0701 - val_loss: 0.0576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0697 - val_loss: 0.0587\n",
      "Epoch 97/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0697 - val_loss: 0.0581\n",
      "Epoch 98/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0693 - val_loss: 0.0588\n",
      "Epoch 99/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0698 - val_loss: 0.0584\n",
      "Epoch 100/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0695 - val_loss: 0.0583\n",
      "Epoch 101/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0700 - val_loss: 0.0585\n",
      "Epoch 102/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0702 - val_loss: 0.0584\n",
      "Epoch 103/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0695 - val_loss: 0.0587\n",
      "Epoch 104/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0694 - val_loss: 0.0583\n",
      "Epoch 105/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0697 - val_loss: 0.0589\n",
      "Epoch 106/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0691 - val_loss: 0.0586\n",
      "Epoch 107/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0695 - val_loss: 0.0588\n",
      "Epoch 108/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0694 - val_loss: 0.0587\n",
      "Epoch 109/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0695 - val_loss: 0.0587\n",
      "Epoch 110/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0696 - val_loss: 0.0589\n",
      "Epoch 111/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0695 - val_loss: 0.0589\n",
      "Epoch 112/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0688 - val_loss: 0.0586\n",
      "Epoch 113/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0690 - val_loss: 0.0589\n",
      "Epoch 114/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0696 - val_loss: 0.0588\n",
      "Epoch 115/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0690 - val_loss: 0.0586\n",
      "Epoch 116/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0692 - val_loss: 0.0591\n",
      "Epoch 117/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0692 - val_loss: 0.0587\n",
      "Epoch 118/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0692 - val_loss: 0.0596\n",
      "Epoch 119/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0689 - val_loss: 0.0591\n",
      "Epoch 120/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0687 - val_loss: 0.0599\n",
      "Epoch 121/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0688 - val_loss: 0.0586\n",
      "Epoch 122/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0691 - val_loss: 0.0590\n",
      "Epoch 123/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0687 - val_loss: 0.0594\n",
      "Epoch 124/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0686 - val_loss: 0.0596\n",
      "Epoch 125/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0685 - val_loss: 0.0596\n",
      "Epoch 126/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0685 - val_loss: 0.0587\n",
      "Epoch 127/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0688 - val_loss: 0.0596\n",
      "Epoch 128/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0687 - val_loss: 0.0595\n",
      "Epoch 129/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0687 - val_loss: 0.0599\n",
      "Epoch 130/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0689 - val_loss: 0.0601\n",
      "118108/118108 [==============================] - 0s 1us/step\n",
      "Fold 1. auc: 0.9761.\n",
      "Fold 3 started at Sun Sep 22 22:52:25 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.3001 - val_loss: 0.1533\n",
      "Epoch 2/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1424 - val_loss: 0.0807\n",
      "Epoch 3/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1093 - val_loss: 0.0649\n",
      "Epoch 4/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1010 - val_loss: 0.0609\n",
      "Epoch 5/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0991 - val_loss: 0.0592\n",
      "Epoch 6/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0961 - val_loss: 0.0586\n",
      "Epoch 7/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0941 - val_loss: 0.0581\n",
      "Epoch 8/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0932 - val_loss: 0.0582\n",
      "Epoch 9/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0917 - val_loss: 0.0584\n",
      "Epoch 10/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0901 - val_loss: 0.0582\n",
      "Epoch 11/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0900 - val_loss: 0.0586\n",
      "Epoch 12/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0894 - val_loss: 0.0588\n",
      "Epoch 13/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0883 - val_loss: 0.0588\n",
      "Epoch 14/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0874 - val_loss: 0.0590\n",
      "Epoch 15/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0875 - val_loss: 0.0592\n",
      "Epoch 16/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0867 - val_loss: 0.0592\n",
      "Epoch 17/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0859 - val_loss: 0.0600\n",
      "Epoch 18/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0852 - val_loss: 0.0589\n",
      "Epoch 19/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0855 - val_loss: 0.0596\n",
      "Epoch 20/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0843 - val_loss: 0.0593\n",
      "Epoch 21/130\n",
      "472432/472432 [==============================] - 3s 7us/step - loss: 0.0837 - val_loss: 0.0590\n",
      "Epoch 22/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0828 - val_loss: 0.0590\n",
      "Epoch 23/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0824 - val_loss: 0.0591\n",
      "Epoch 24/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0825 - val_loss: 0.0594\n",
      "Epoch 25/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0821 - val_loss: 0.0586\n",
      "Epoch 26/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0818 - val_loss: 0.0585\n",
      "Epoch 27/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0818 - val_loss: 0.0592\n",
      "Epoch 28/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0813 - val_loss: 0.0583\n",
      "Epoch 29/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0811 - val_loss: 0.0586\n",
      "Epoch 30/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0805 - val_loss: 0.0587\n",
      "Epoch 31/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0794 - val_loss: 0.0577\n",
      "Epoch 32/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0795 - val_loss: 0.0584\n",
      "Epoch 33/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0801 - val_loss: 0.0586\n",
      "Epoch 34/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0791 - val_loss: 0.0578\n",
      "Epoch 35/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0792 - val_loss: 0.0580\n",
      "Epoch 36/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0790 - val_loss: 0.0575\n",
      "Epoch 37/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0788 - val_loss: 0.0580\n",
      "Epoch 38/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0785 - val_loss: 0.0576\n",
      "Epoch 39/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0783 - val_loss: 0.0579\n",
      "Epoch 40/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0777 - val_loss: 0.0571\n",
      "Epoch 41/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0779 - val_loss: 0.0570\n",
      "Epoch 42/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0767 - val_loss: 0.0566\n",
      "Epoch 43/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0777 - val_loss: 0.0566\n",
      "Epoch 44/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0771 - val_loss: 0.0566\n",
      "Epoch 45/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0765 - val_loss: 0.0564\n",
      "Epoch 46/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0765 - val_loss: 0.0566\n",
      "Epoch 47/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0767 - val_loss: 0.0561\n",
      "Epoch 48/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0761 - val_loss: 0.0563\n",
      "Epoch 49/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0761 - val_loss: 0.0561\n",
      "Epoch 50/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0762 - val_loss: 0.0561\n",
      "Epoch 51/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0769 - val_loss: 0.0560\n",
      "Epoch 52/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0763 - val_loss: 0.0558\n",
      "Epoch 53/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0754 - val_loss: 0.0555\n",
      "Epoch 54/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0757 - val_loss: 0.0556\n",
      "Epoch 55/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0754 - val_loss: 0.0555\n",
      "Epoch 56/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0757 - val_loss: 0.0556\n",
      "Epoch 57/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0750 - val_loss: 0.0553\n",
      "Epoch 58/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0751 - val_loss: 0.0550\n",
      "Epoch 59/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0555\n",
      "Epoch 60/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0550\n",
      "Epoch 61/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0549\n",
      "Epoch 62/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0745 - val_loss: 0.0549\n",
      "Epoch 63/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0747 - val_loss: 0.0550\n",
      "Epoch 64/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0548\n",
      "Epoch 65/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0548\n",
      "Epoch 66/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0738 - val_loss: 0.0549\n",
      "Epoch 67/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0548\n",
      "Epoch 68/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0739 - val_loss: 0.0547\n",
      "Epoch 69/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0734 - val_loss: 0.0541\n",
      "Epoch 70/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0736 - val_loss: 0.0545\n",
      "Epoch 71/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0732 - val_loss: 0.0545\n",
      "Epoch 72/130\n",
      "472432/472432 [==============================] - 2s 5us/step - loss: 0.0726 - val_loss: 0.0541\n",
      "Epoch 73/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0730 - val_loss: 0.0543\n",
      "Epoch 74/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0730 - val_loss: 0.0543\n",
      "Epoch 75/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0727 - val_loss: 0.0540\n",
      "Epoch 76/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0722 - val_loss: 0.0539\n",
      "Epoch 77/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0729 - val_loss: 0.0541\n",
      "Epoch 78/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0728 - val_loss: 0.0539\n",
      "Epoch 79/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0723 - val_loss: 0.0539\n",
      "Epoch 80/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0539\n",
      "Epoch 81/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0538\n",
      "Epoch 82/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0726 - val_loss: 0.0540\n",
      "Epoch 83/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0722 - val_loss: 0.0538\n",
      "Epoch 84/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0722 - val_loss: 0.0543\n",
      "Epoch 85/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0719 - val_loss: 0.0540\n",
      "Epoch 86/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0719 - val_loss: 0.0539\n",
      "Epoch 87/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0716 - val_loss: 0.0536\n",
      "Epoch 88/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0719 - val_loss: 0.0542\n",
      "Epoch 89/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0722 - val_loss: 0.0538\n",
      "Epoch 90/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0718 - val_loss: 0.0538\n",
      "Epoch 91/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0716 - val_loss: 0.0537\n",
      "Epoch 92/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0722 - val_loss: 0.0539\n",
      "Epoch 93/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0718 - val_loss: 0.0536\n",
      "Epoch 94/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0719 - val_loss: 0.0539\n",
      "Epoch 95/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0712 - val_loss: 0.0539\n",
      "Epoch 96/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0710 - val_loss: 0.0541\n",
      "Epoch 97/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0714 - val_loss: 0.0540\n",
      "Epoch 98/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0711 - val_loss: 0.0542\n",
      "Epoch 99/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0535\n",
      "Epoch 100/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0707 - val_loss: 0.0539\n",
      "Epoch 101/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0712 - val_loss: 0.0540\n",
      "Epoch 102/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0712 - val_loss: 0.0540\n",
      "Epoch 103/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0540\n",
      "Epoch 104/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0711 - val_loss: 0.0540\n",
      "Epoch 105/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0706 - val_loss: 0.0544\n",
      "Epoch 106/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0712 - val_loss: 0.0538\n",
      "Epoch 107/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0540\n",
      "Epoch 108/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0707 - val_loss: 0.0544\n",
      "Epoch 109/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0710 - val_loss: 0.0541\n",
      "Epoch 110/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0704 - val_loss: 0.0535\n",
      "Epoch 111/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0704 - val_loss: 0.0539\n",
      "Epoch 112/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0706 - val_loss: 0.0536\n",
      "Epoch 113/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0544\n",
      "Epoch 114/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0704 - val_loss: 0.0545\n",
      "Epoch 115/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0695 - val_loss: 0.0543\n",
      "Epoch 117/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0540\n",
      "Epoch 118/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0700 - val_loss: 0.0542\n",
      "Epoch 119/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0705 - val_loss: 0.0544\n",
      "Epoch 120/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0703 - val_loss: 0.0543\n",
      "Epoch 121/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0698 - val_loss: 0.0544\n",
      "Epoch 122/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0544\n",
      "Epoch 123/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0537\n",
      "Epoch 124/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0701 - val_loss: 0.0540\n",
      "Epoch 125/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0543\n",
      "Epoch 126/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0541\n",
      "Epoch 127/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0701 - val_loss: 0.0542\n",
      "Epoch 128/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0702 - val_loss: 0.0545\n",
      "Epoch 129/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0699 - val_loss: 0.0547\n",
      "Epoch 130/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0703 - val_loss: 0.0539\n",
      "118108/118108 [==============================] - 0s 1us/step\n",
      "Fold 2. auc: 0.9761.\n",
      "Fold 4 started at Sun Sep 22 22:56:03 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.2723 - val_loss: 0.1498\n",
      "Epoch 2/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.1357 - val_loss: 0.0830\n",
      "Epoch 3/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.1087 - val_loss: 0.0664\n",
      "Epoch 4/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.1000 - val_loss: 0.0619\n",
      "Epoch 5/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0968 - val_loss: 0.0602\n",
      "Epoch 6/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0952 - val_loss: 0.0596\n",
      "Epoch 7/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0926 - val_loss: 0.0591\n",
      "Epoch 8/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0914 - val_loss: 0.0590\n",
      "Epoch 9/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0908 - val_loss: 0.0591\n",
      "Epoch 10/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0891 - val_loss: 0.0592\n",
      "Epoch 11/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0881 - val_loss: 0.0592\n",
      "Epoch 12/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0883 - val_loss: 0.0594\n",
      "Epoch 13/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0872 - val_loss: 0.0600\n",
      "Epoch 14/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0865 - val_loss: 0.0599\n",
      "Epoch 15/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0855 - val_loss: 0.0599\n",
      "Epoch 16/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0854 - val_loss: 0.0600\n",
      "Epoch 17/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0849 - val_loss: 0.0602\n",
      "Epoch 18/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0849 - val_loss: 0.0606\n",
      "Epoch 19/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0846 - val_loss: 0.0603\n",
      "Epoch 20/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0833 - val_loss: 0.0602\n",
      "Epoch 21/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0841 - val_loss: 0.0608\n",
      "Epoch 22/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0821 - val_loss: 0.0602\n",
      "Epoch 23/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0838 - val_loss: 0.0606\n",
      "Epoch 24/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0826 - val_loss: 0.0606\n",
      "Epoch 25/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0817 - val_loss: 0.0605\n",
      "Epoch 26/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0824 - val_loss: 0.0606\n",
      "Epoch 27/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0813 - val_loss: 0.0602\n",
      "Epoch 28/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0810 - val_loss: 0.0601\n",
      "Epoch 29/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0811 - val_loss: 0.0597\n",
      "Epoch 30/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0797 - val_loss: 0.0592\n",
      "Epoch 31/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0801 - val_loss: 0.0596\n",
      "Epoch 32/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0793 - val_loss: 0.0589\n",
      "Epoch 33/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0795 - val_loss: 0.0587\n",
      "Epoch 34/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0791 - val_loss: 0.0585\n",
      "Epoch 35/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0789 - val_loss: 0.0589\n",
      "Epoch 36/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0788 - val_loss: 0.0581\n",
      "Epoch 37/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0781 - val_loss: 0.0582\n",
      "Epoch 38/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0785 - val_loss: 0.0579\n",
      "Epoch 39/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0771 - val_loss: 0.0574\n",
      "Epoch 40/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0765 - val_loss: 0.0569\n",
      "Epoch 41/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0771 - val_loss: 0.0571\n",
      "Epoch 42/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0772 - val_loss: 0.0572\n",
      "Epoch 43/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0771 - val_loss: 0.0567\n",
      "Epoch 44/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0766 - val_loss: 0.0570\n",
      "Epoch 45/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0758 - val_loss: 0.0565\n",
      "Epoch 46/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0764 - val_loss: 0.0570\n",
      "Epoch 47/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0760 - val_loss: 0.0561\n",
      "Epoch 48/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0761 - val_loss: 0.0563\n",
      "Epoch 49/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0757 - val_loss: 0.0563\n",
      "Epoch 50/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0753 - val_loss: 0.0560\n",
      "Epoch 51/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0755 - val_loss: 0.0559\n",
      "Epoch 52/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0750 - val_loss: 0.0559\n",
      "Epoch 53/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0755 - val_loss: 0.0558\n",
      "Epoch 54/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0751 - val_loss: 0.0560\n",
      "Epoch 55/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0746 - val_loss: 0.0557\n",
      "Epoch 56/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0748 - val_loss: 0.0555\n",
      "Epoch 57/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0559\n",
      "Epoch 58/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0745 - val_loss: 0.0556\n",
      "Epoch 59/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0742 - val_loss: 0.0551\n",
      "Epoch 60/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0748 - val_loss: 0.0554\n",
      "Epoch 61/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0738 - val_loss: 0.0548\n",
      "Epoch 62/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0735 - val_loss: 0.0546\n",
      "Epoch 63/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0735 - val_loss: 0.0545\n",
      "Epoch 64/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0735 - val_loss: 0.0548\n",
      "Epoch 65/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0739 - val_loss: 0.0546\n",
      "Epoch 66/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0734 - val_loss: 0.0545\n",
      "Epoch 67/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0729 - val_loss: 0.0549\n",
      "Epoch 68/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0544\n",
      "Epoch 69/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0734 - val_loss: 0.0545\n",
      "Epoch 70/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0734 - val_loss: 0.0544\n",
      "Epoch 71/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0730 - val_loss: 0.0545\n",
      "Epoch 72/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0723 - val_loss: 0.0542\n",
      "Epoch 73/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0723 - val_loss: 0.0541\n",
      "Epoch 74/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0732 - val_loss: 0.0545\n",
      "Epoch 75/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0726 - val_loss: 0.0541\n",
      "Epoch 76/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0724 - val_loss: 0.0541\n",
      "Epoch 77/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0725 - val_loss: 0.0542\n",
      "Epoch 78/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0729 - val_loss: 0.0545\n",
      "Epoch 79/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0718 - val_loss: 0.0540\n",
      "Epoch 80/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0714 - val_loss: 0.0542\n",
      "Epoch 81/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0721 - val_loss: 0.0541\n",
      "Epoch 82/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0720 - val_loss: 0.0541\n",
      "Epoch 83/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0721 - val_loss: 0.0541\n",
      "Epoch 84/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0719 - val_loss: 0.0541\n",
      "Epoch 85/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0714 - val_loss: 0.0540\n",
      "Epoch 86/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0715 - val_loss: 0.0538\n",
      "Epoch 87/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0719 - val_loss: 0.0542\n",
      "Epoch 88/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0710 - val_loss: 0.0541\n",
      "Epoch 89/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0710 - val_loss: 0.0539\n",
      "Epoch 90/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0712 - val_loss: 0.0542\n",
      "Epoch 91/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0713 - val_loss: 0.0540\n",
      "Epoch 92/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0710 - val_loss: 0.0543\n",
      "Epoch 93/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0712 - val_loss: 0.0542\n",
      "Epoch 94/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0707 - val_loss: 0.0543\n",
      "Epoch 95/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0709 - val_loss: 0.0542\n",
      "Epoch 96/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0711 - val_loss: 0.0542\n",
      "Epoch 97/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0712 - val_loss: 0.0541\n",
      "Epoch 98/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0712 - val_loss: 0.0545\n",
      "Epoch 99/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0704 - val_loss: 0.0544\n",
      "Epoch 100/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0707 - val_loss: 0.0543\n",
      "Epoch 101/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0709 - val_loss: 0.0548\n",
      "Epoch 102/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0546\n",
      "Epoch 103/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0706 - val_loss: 0.0545\n",
      "Epoch 104/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0542\n",
      "Epoch 105/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0703 - val_loss: 0.0543\n",
      "Epoch 106/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0704 - val_loss: 0.0547\n",
      "Epoch 107/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0706 - val_loss: 0.0546\n",
      "Epoch 108/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0705 - val_loss: 0.0546\n",
      "Epoch 109/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0699 - val_loss: 0.0548\n",
      "Epoch 110/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0700 - val_loss: 0.0547\n",
      "Epoch 111/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0700 - val_loss: 0.0545\n",
      "Epoch 112/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0702 - val_loss: 0.0547\n",
      "Epoch 113/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0704 - val_loss: 0.0549\n",
      "Epoch 114/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0701 - val_loss: 0.0546\n",
      "Epoch 115/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0697 - val_loss: 0.0547\n",
      "Epoch 116/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0697 - val_loss: 0.0543\n",
      "Epoch 117/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0701 - val_loss: 0.0550\n",
      "Epoch 118/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0705 - val_loss: 0.0552\n",
      "Epoch 119/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0698 - val_loss: 0.0548\n",
      "Epoch 120/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0695 - val_loss: 0.0547\n",
      "Epoch 121/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0693 - val_loss: 0.0548\n",
      "Epoch 122/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0699 - val_loss: 0.0552\n",
      "Epoch 123/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0698 - val_loss: 0.0550\n",
      "Epoch 124/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0701 - val_loss: 0.0549\n",
      "Epoch 125/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0700 - val_loss: 0.0557\n",
      "Epoch 126/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0697 - val_loss: 0.0554\n",
      "Epoch 127/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0699 - val_loss: 0.0548\n",
      "Epoch 128/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0695 - val_loss: 0.0552\n",
      "Epoch 129/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0693 - val_loss: 0.0550\n",
      "Epoch 130/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0695 - val_loss: 0.0553\n",
      "118108/118108 [==============================] - 0s 1us/step\n",
      "Fold 3. auc: 0.9789.\n",
      "Fold 5 started at Sun Sep 22 22:59:33 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.3648 - val_loss: 0.1530\n",
      "Epoch 2/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1457 - val_loss: 0.0990\n",
      "Epoch 3/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.1149 - val_loss: 0.0752\n",
      "Epoch 4/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.1042 - val_loss: 0.0667\n",
      "Epoch 5/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0996 - val_loss: 0.0631\n",
      "Epoch 6/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0978 - val_loss: 0.0614\n",
      "Epoch 7/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0958 - val_loss: 0.0605\n",
      "Epoch 8/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0953 - val_loss: 0.0599\n",
      "Epoch 9/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0938 - val_loss: 0.0594\n",
      "Epoch 10/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0917 - val_loss: 0.0590\n",
      "Epoch 11/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0913 - val_loss: 0.0588\n",
      "Epoch 12/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0906 - val_loss: 0.0585\n",
      "Epoch 13/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0903 - val_loss: 0.0585\n",
      "Epoch 14/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0891 - val_loss: 0.0585\n",
      "Epoch 15/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0883 - val_loss: 0.0583\n",
      "Epoch 16/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0880 - val_loss: 0.0583\n",
      "Epoch 17/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0879 - val_loss: 0.0587\n",
      "Epoch 18/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0865 - val_loss: 0.0584\n",
      "Epoch 19/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0863 - val_loss: 0.0587\n",
      "Epoch 20/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0850 - val_loss: 0.0589\n",
      "Epoch 21/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0850 - val_loss: 0.0588\n",
      "Epoch 22/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0843 - val_loss: 0.0587\n",
      "Epoch 23/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0845 - val_loss: 0.0590\n",
      "Epoch 24/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0838 - val_loss: 0.0586\n",
      "Epoch 25/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0835 - val_loss: 0.0590\n",
      "Epoch 26/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0833 - val_loss: 0.0591\n",
      "Epoch 27/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0834 - val_loss: 0.0594\n",
      "Epoch 28/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0818 - val_loss: 0.0591\n",
      "Epoch 29/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0830 - val_loss: 0.0591\n",
      "Epoch 30/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0826 - val_loss: 0.0594\n",
      "Epoch 31/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0815 - val_loss: 0.0588\n",
      "Epoch 32/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0809 - val_loss: 0.0587\n",
      "Epoch 33/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0811 - val_loss: 0.0587\n",
      "Epoch 34/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0811 - val_loss: 0.0591\n",
      "Epoch 35/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0808 - val_loss: 0.0585\n",
      "Epoch 36/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0800 - val_loss: 0.0578\n",
      "Epoch 37/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0795 - val_loss: 0.0580\n",
      "Epoch 38/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0796 - val_loss: 0.0581\n",
      "Epoch 39/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0798 - val_loss: 0.0580\n",
      "Epoch 40/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0792 - val_loss: 0.0577\n",
      "Epoch 41/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0793 - val_loss: 0.0576\n",
      "Epoch 42/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0791 - val_loss: 0.0574\n",
      "Epoch 43/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0784 - val_loss: 0.0570\n",
      "Epoch 44/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0779 - val_loss: 0.0572\n",
      "Epoch 45/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0786 - val_loss: 0.0569\n",
      "Epoch 46/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0780 - val_loss: 0.0569\n",
      "Epoch 47/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0784 - val_loss: 0.0567\n",
      "Epoch 48/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0776 - val_loss: 0.0563\n",
      "Epoch 49/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0777 - val_loss: 0.0568\n",
      "Epoch 50/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0772 - val_loss: 0.0563\n",
      "Epoch 51/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0777 - val_loss: 0.0563\n",
      "Epoch 52/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0778 - val_loss: 0.0562\n",
      "Epoch 53/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0766 - val_loss: 0.0554\n",
      "Epoch 54/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0769 - val_loss: 0.0563\n",
      "Epoch 55/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0764 - val_loss: 0.0555\n",
      "Epoch 56/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0768 - val_loss: 0.0556\n",
      "Epoch 57/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0764 - val_loss: 0.0556\n",
      "Epoch 58/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0763 - val_loss: 0.0554\n",
      "Epoch 59/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0765 - val_loss: 0.0552\n",
      "Epoch 60/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0761 - val_loss: 0.0552\n",
      "Epoch 61/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0761 - val_loss: 0.0551\n",
      "Epoch 62/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0763 - val_loss: 0.0551\n",
      "Epoch 63/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0765 - val_loss: 0.0553\n",
      "Epoch 64/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0761 - val_loss: 0.0550\n",
      "Epoch 65/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0764 - val_loss: 0.0548\n",
      "Epoch 66/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0755 - val_loss: 0.0543\n",
      "Epoch 67/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0758 - val_loss: 0.0544\n",
      "Epoch 68/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0756 - val_loss: 0.0547\n",
      "Epoch 69/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0747 - val_loss: 0.0545\n",
      "Epoch 70/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0750 - val_loss: 0.0541\n",
      "Epoch 71/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0751 - val_loss: 0.0539\n",
      "Epoch 72/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0743 - val_loss: 0.0536\n",
      "Epoch 73/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0539\n",
      "Epoch 74/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0746 - val_loss: 0.0541\n",
      "Epoch 75/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0539\n",
      "Epoch 76/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0539\n",
      "Epoch 77/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0533\n",
      "Epoch 78/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0742 - val_loss: 0.0539\n",
      "Epoch 79/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0741 - val_loss: 0.0534\n",
      "Epoch 80/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0744 - val_loss: 0.0539\n",
      "Epoch 81/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0743 - val_loss: 0.0530\n",
      "Epoch 82/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0747 - val_loss: 0.0534\n",
      "Epoch 83/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0742 - val_loss: 0.0534\n",
      "Epoch 84/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0533\n",
      "Epoch 85/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0739 - val_loss: 0.0532\n",
      "Epoch 86/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0536\n",
      "Epoch 87/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0731 - val_loss: 0.0531\n",
      "Epoch 88/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0734 - val_loss: 0.0530\n",
      "Epoch 89/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0736 - val_loss: 0.0529\n",
      "Epoch 90/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0736 - val_loss: 0.0528\n",
      "Epoch 91/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0726 - val_loss: 0.0524\n",
      "Epoch 92/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0728 - val_loss: 0.0527\n",
      "Epoch 93/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0726 - val_loss: 0.0524\n",
      "Epoch 94/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0726 - val_loss: 0.0527\n",
      "Epoch 95/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0732 - val_loss: 0.0527\n",
      "Epoch 96/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0728 - val_loss: 0.0528\n",
      "Epoch 97/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0727 - val_loss: 0.0525\n",
      "Epoch 98/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0729 - val_loss: 0.0527\n",
      "Epoch 99/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0728 - val_loss: 0.0528\n",
      "Epoch 100/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0721 - val_loss: 0.0525\n",
      "Epoch 101/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0722 - val_loss: 0.0520\n",
      "Epoch 102/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0718 - val_loss: 0.0525\n",
      "Epoch 103/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0717 - val_loss: 0.0519\n",
      "Epoch 104/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0720 - val_loss: 0.0520\n",
      "Epoch 105/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0719 - val_loss: 0.0523\n",
      "Epoch 106/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0720 - val_loss: 0.0522\n",
      "Epoch 107/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0717 - val_loss: 0.0522\n",
      "Epoch 108/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0716 - val_loss: 0.0523\n",
      "Epoch 109/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0718 - val_loss: 0.0524\n",
      "Epoch 110/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0716 - val_loss: 0.0523\n",
      "Epoch 111/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0718 - val_loss: 0.0521\n",
      "Epoch 112/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0721 - val_loss: 0.0525\n",
      "Epoch 113/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0716 - val_loss: 0.0520\n",
      "Epoch 114/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0717 - val_loss: 0.0524\n",
      "Epoch 115/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0712 - val_loss: 0.0521\n",
      "Epoch 116/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0716 - val_loss: 0.0522\n",
      "Epoch 117/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0713 - val_loss: 0.0518\n",
      "Epoch 118/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0707 - val_loss: 0.0524\n",
      "Epoch 119/130\n",
      "472432/472432 [==============================] - 1s 3us/step - loss: 0.0710 - val_loss: 0.0524\n",
      "Epoch 120/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0713 - val_loss: 0.0520\n",
      "Epoch 121/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0708 - val_loss: 0.0521\n",
      "Epoch 122/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0712 - val_loss: 0.0522\n",
      "Epoch 123/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0709 - val_loss: 0.0521\n",
      "Epoch 124/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0708 - val_loss: 0.0524\n",
      "Epoch 125/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0708 - val_loss: 0.0521\n",
      "Epoch 126/130\n",
      "472432/472432 [==============================] - 2s 4us/step - loss: 0.0709 - val_loss: 0.0524\n",
      "Epoch 127/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0710 - val_loss: 0.0525\n",
      "Epoch 128/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0709 - val_loss: 0.0524\n",
      "Epoch 129/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0706 - val_loss: 0.0520\n",
      "Epoch 130/130\n",
      "472432/472432 [==============================] - 2s 3us/step - loss: 0.0710 - val_loss: 0.0525\n",
      "118108/118108 [==============================] - 0s 1us/step\n",
      "Fold 4. auc: 0.9769.\n",
      "CV mean score: 0.9749, std: 0.0043.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "result_dict_keras = train_model_classification(model=NNModel_maker, \n",
    "                                             X=X,\n",
    "                                             X_test=test,\n",
    "                                             y=y, params=params, folds=folds,\n",
    "                                             model_type=train_options['model_type'], \n",
    "                                             eval_metric=train_options['eval_metric'],\n",
    "                                             plot_feature_importance=True,\n",
    "                                             averaging=train_options['averaging'],\n",
    "                                             groups=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T20:03:09.857253Z",
     "start_time": "2019-09-22T20:03:09.569213Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(result_dict_keras, open(f'{p.working_folder}/results_dict_keras.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T20:03:12.314927Z",
     "start_time": "2019-09-22T20:03:12.052252Z"
    }
   },
   "outputs": [],
   "source": [
    "# test = test.sort_values('Date')\n",
    "test['prediction'] = result_dict_keras['prediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T20:03:17.376402Z",
     "start_time": "2019-09-22T20:03:14.638892Z"
    }
   },
   "outputs": [],
   "source": [
    "# folder_path = './data/'\n",
    "sub = pd.read_csv(f'{data_path}/sample_submission.csv')\n",
    "sub['isFraud'] = pd.merge(sub, test['prediction'], on='TransactionID')['prediction']\n",
    "sub.to_csv(f'{p.working_folder}/submission_dynamic_20_keras.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
