{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:31.759076Z",
     "start_time": "2019-09-01T17:06:26.487233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import  Adam\n",
    "import keras.backend as k\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# pd.options.display.precision = 15\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import time\n",
    "# import datetime\n",
    "# from catboost import CatBoostRegressor\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "# from sklearn import metrics\n",
    "# from sklearn import linear_model\n",
    "import gc\n",
    "import pickle\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import eli5\n",
    "# import shap\n",
    "# from IPython.display import HTML\n",
    "# import json\n",
    "# import altair as alt\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "gc.collect()\n",
    "# alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:34.553981Z",
     "start_time": "2019-09-01T17:06:34.336982Z"
    }
   },
   "outputs": [],
   "source": [
    "main_path = r'd:\\Documents\\Private\\Kaggle\\IEEEFraud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:20:59.051411Z",
     "start_time": "2019-09-01T17:20:58.725422Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(main_path)\n",
    "from BayDS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:41.481317Z",
     "start_time": "2019-09-01T17:06:41.250316Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '31.08'\n",
    "main_learning_folder = main_path+'/Snapshots/'+experiment_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:28:31.840483Z",
     "start_time": "2019-09-01T16:28:26.730472Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_pickle(f'{main_learning_folder}/train.pkl')\n",
    "test = pd.read_pickle(f'{main_learning_folder}/test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:28:53.594573Z",
     "start_time": "2019-09-01T16:28:31.842452Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\n",
    "y = train.sort_values('TransactionDT')['isFraud'].astype(np.uint8)\n",
    "test = test.sort_values('TransactionDT').drop(['TransactionDT'], axis=1)\n",
    "\n",
    "X.drop('Date', axis=1, inplace=True)\n",
    "X=X.astype(np.float32)\n",
    "test.drop('Date', axis=1, inplace=True)\n",
    "test=test.astype(np.float32)\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:28:53.926571Z",
     "start_time": "2019-09-01T16:28:53.597572Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:29:00.626777Z",
     "start_time": "2019-09-01T16:28:53.929571Z"
    }
   },
   "outputs": [],
   "source": [
    "merg = pd.read_pickle(f'{main_learning_folder}/catboostencoder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:49.862187Z",
     "start_time": "2019-09-01T17:06:49.620003Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_anya = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1',\n",
    "       'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15',\n",
    "       'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_33',\n",
    "       'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo',\n",
    "       'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin',\n",
    "       'R_emaildomain_suffix', 'device_name', 'device_version', 'OS_id_30',\n",
    "       'version_id_30', 'browser_id_31', 'version_id_31', 'screen_width',\n",
    "       'screen_height', 'uid', 'uid2', 'uid3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:53.087084Z",
     "start_time": "2019-09-01T17:06:52.836019Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_vasya = [\"id_13\", \"id_14\", \"id_17\", \"id_18\", \"id_19\", \"id_20\", \"id_21\", \"id_22\", \"id_23\", \n",
    " \"id_24\", \"id_25\", \"id_26\", \"id_27\", \"id_30\", \"id_31\", \"id_33\", \"DeviceType\", \n",
    " \"DeviceInfo\", \"P_emaildomain\", \"R_emaildomain\", \"card1\", \"card2\", \"card3\", \n",
    " \"card5\", \"addr1\", \"addr2\", \"P_emaildomain_bin\", \"P_emaildomain_suffix\", \n",
    " \"R_emaildomain_bin\", \"R_emaildomain_suffix\", \"M1\", \"M2\", \"M3\", \"M4\", \"M5\", \n",
    " \"M6\", \"M7\", \"M8\", \"M9\", \"card4\", \"card6\", \"ProductCD\", \"id_12\", \"id_15\", \n",
    " \"id_16\", \"id_28\", \"id_29\", \"id_32\", \"id_34\", \"id_35\", \"id_36\", \"id_37\", \"id_38\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:56.299513Z",
     "start_time": "2019-09-01T17:06:56.052524Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_features = list(set(cat_vasya) | set(cat_anya))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:06:59.857571Z",
     "start_time": "2019-09-01T17:06:59.256556Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-173fed91dc89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "num_features = list(set(X.columns) - set(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:29:23.142159Z",
     "start_time": "2019-09-01T16:29:22.891144Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:29:40.617444Z",
     "start_time": "2019-09-01T16:29:32.517727Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for col in cat_features:\n",
    "#     alld = pd.concat([X[col],test[col]])\n",
    "#     num_values = len(alld.unique()) \n",
    "#     if num_values > 20:\n",
    "#         print(col, '- Big categroical')\n",
    "#         X[col] = merg[col].loc[X.index]\n",
    "#         test[col]= merg[col].loc[test.index]\n",
    "#     elif num_values>2:\n",
    "#         print(col,'- one-hot')\n",
    "#         temp = pd.get_dummies(alld,prefix=f'{col}_')\n",
    "#         X=pd.concat([X,temp.loc[X.index]],axis=1)\n",
    "#         X.drop([col],axis=1,inplace=True)\n",
    "#         test=pd.concat([test,temp.loc[test.index]],axis=1)\n",
    "#         test.drop([col],axis=1,inplace=True)\n",
    "        \n",
    "# X.to_pickle(f'{main_learning_folder}/X_After_encoding.pkl')\n",
    "# test.to_pickle(f'{main_learning_folder}/test_after_encoding.pkl')\n",
    "\n",
    "X = pd.read_pickle(f'{main_learning_folder}/X_After_encoding.pkl')\n",
    "test = pd.read_pickle(f'{main_learning_folder}/test_after_encoding.pkl')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:01:49.519729Z",
     "start_time": "2019-09-01T17:01:48.361694Z"
    }
   },
   "outputs": [],
   "source": [
    "del merg\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:32:13.247999Z",
     "start_time": "2019-09-01T16:29:53.096212Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in num_features:\n",
    "    print(column)\n",
    "    col = pd.concat([X[column],test[column]])\n",
    "    filtered = col[~col.isin([np.inf, -np.inf])].dropna()\n",
    "    vmax = filtered.max()\n",
    "    vmin = filtered.min()\n",
    "    vmean = filtered.mean()\n",
    "    col = col.replace({\n",
    "        np.inf:vmax,\n",
    "        -np.inf:vmin\n",
    "       }).fillna(vmean)\n",
    "    if vmax > 100 and vmin >= 0:\n",
    "        col = np.log1p(col)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    col.values[...] = scaler.fit_transform(col.values.reshape(-1,1)).flatten()\n",
    "    \n",
    "    X[column] = col[X.index]\n",
    "    test[column] = col[test.index]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:05:34.751422Z",
     "start_time": "2019-09-01T17:05:34.465426Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:32:37.687099Z",
     "start_time": "2019-09-01T16:32:21.152017Z"
    }
   },
   "outputs": [],
   "source": [
    "X.to_pickle(f'{main_learning_folder}/X_encoded_scaled.pkl')\n",
    "y.to_pickle(f'{main_learning_folder}/y.pkl')\n",
    "test.to_pickle(f'{main_learning_folder}/test_encoded_scaled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:07:10.196891Z",
     "start_time": "2019-09-01T17:07:04.130151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start here\n",
    "y = pd.read_pickle(f'{main_learning_folder}/y.pkl')\n",
    "X = pd.read_pickle(f'{main_learning_folder}/X_encoded_scaled.pkl')\n",
    "test = pd.read_pickle(f'{main_learning_folder}/test_encoded_scaled.pkl')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:07:16.735743Z",
     "start_time": "2019-09-01T17:07:16.468739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting model_folder\n",
    "model_name = 'keras-2'\n",
    "model_folder = f'{main_learning_folder}/{model_name}'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:07:23.049831Z",
     "start_time": "2019-09-01T17:07:22.808843Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "folds = KFold(n_splits=n_fold)\n",
    "# folds = GroupKFold(n_splits=5)\n",
    "# groups = pd.read_pickle('./groups.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T16:33:02.653823Z",
     "start_time": "2019-09-01T16:33:02.392793Z"
    }
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:07:29.232881Z",
     "start_time": "2019-09-01T17:07:28.990851Z"
    }
   },
   "outputs": [],
   "source": [
    "def NNModel_maker():\n",
    "    k.clear_session()\n",
    "    \n",
    "#     categorical_inputs = []\n",
    "#     for cat in categorical:\n",
    "#         categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "\n",
    "#     categorical_embeddings = []\n",
    "#     for i, cat in enumerate(categorical):\n",
    "#         categorical_embeddings.append(\n",
    "#             Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \\\n",
    "#                       \"_embed\")(categorical_inputs[i]))\n",
    "\n",
    "#     categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n",
    "# \n",
    "    numerical_inputs = Input(shape=[X.shape[1]], name = 'all')\n",
    "    numerical_logits = Dropout(.3)(numerical_inputs)\n",
    "  \n",
    "    x = numerical_logits\n",
    "\n",
    "    x = Dense(200, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(100, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    out = Dense(1, activation = 'sigmoid')(x)    \n",
    "\n",
    "    model = Model(inputs= [numerical_inputs],outputs=out)\n",
    "    loss = \"binary_crossentropy\"\n",
    "    model.compile(optimizer=Adam(lr = 0.0003), loss = loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': 8000,\n",
    "    'epochs': 30,\n",
    "    'verbose': True,\n",
    "         }\n",
    "train_options = {\n",
    "    \"model_type\":'keras',\n",
    "    \"params\": params,\n",
    "    \"eval_metric\":'auc',\n",
    "    'averaging': 'usual',\n",
    "    'use_groups': False,\n",
    "    'fold_name': folds.__class__.__name__,\n",
    "    'n_splits': n_fold\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T17:07:35.481905Z",
     "start_time": "2019-09-01T17:07:35.244881Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f'{model_folder}/training_params.json', 'w') as f:\n",
    "    q = json.dumps(train_options,indent=2)\n",
    "    f.write(q)\n",
    "# NNModel.save(f'{model_folder}/keras.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-01T17:22:21.021Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Sep  1 20:22:23 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/30\n",
      "472432/472432 [==============================] - 23s 49us/step - loss: 0.2062 - val_loss: 0.1043\n",
      "Epoch 2/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1286 - val_loss: 0.1084\n",
      "Epoch 3/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1232 - val_loss: 0.1068\n",
      "Epoch 4/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1184 - val_loss: 0.1056\n",
      "Epoch 5/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1155 - val_loss: 0.1016\n",
      "Epoch 6/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1125 - val_loss: 0.0986\n",
      "Epoch 7/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1105 - val_loss: 0.0992\n",
      "Epoch 8/30\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.1080 - val_loss: 0.0974\n",
      "Epoch 9/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1062 - val_loss: 0.0932\n",
      "Epoch 10/30\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.1045 - val_loss: 0.0922\n",
      "Epoch 11/30\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.1026 - val_loss: 0.0894\n",
      "Epoch 12/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1011 - val_loss: 0.0855\n",
      "Epoch 13/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0996 - val_loss: 0.0861\n",
      "Epoch 14/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0982 - val_loss: 0.0830\n",
      "Epoch 15/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0972 - val_loss: 0.0820\n",
      "Epoch 16/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0955 - val_loss: 0.0793\n",
      "Epoch 17/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0945 - val_loss: 0.0777\n",
      "Epoch 18/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0936 - val_loss: 0.0801\n",
      "Epoch 19/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0925 - val_loss: 0.0782\n",
      "Epoch 20/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0913 - val_loss: 0.0769\n",
      "Epoch 21/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0909 - val_loss: 0.0756\n",
      "Epoch 22/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0897 - val_loss: 0.0759\n",
      "Epoch 23/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0892 - val_loss: 0.0776\n",
      "Epoch 24/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0883 - val_loss: 0.0756\n",
      "Epoch 25/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0873 - val_loss: 0.0739\n",
      "Epoch 26/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0867 - val_loss: 0.0743\n",
      "Epoch 27/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0861 - val_loss: 0.0727\n",
      "Epoch 28/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0859 - val_loss: 0.0759\n",
      "Epoch 29/30\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0851 - val_loss: 0.0741\n",
      "Epoch 30/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0846 - val_loss: 0.0746\n",
      "118108/118108 [==============================] - 1s 12us/step\n",
      "Fold 0. auc: 0.9215.\n",
      "Fold 2 started at Sun Sep  1 20:33:31 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/30\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.1526 - val_loss: 0.1191\n",
      "Epoch 2/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1168 - val_loss: 0.1137\n",
      "Epoch 3/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1104 - val_loss: 0.1103\n",
      "Epoch 4/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1063 - val_loss: 0.1076\n",
      "Epoch 5/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1034 - val_loss: 0.1057\n",
      "Epoch 6/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1011 - val_loss: 0.1044\n",
      "Epoch 7/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0985 - val_loss: 0.1013\n",
      "Epoch 8/30\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0969 - val_loss: 0.0998\n",
      "Epoch 9/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0954 - val_loss: 0.0987\n",
      "Epoch 10/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0937 - val_loss: 0.0969\n",
      "Epoch 11/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0921 - val_loss: 0.0949\n",
      "Epoch 12/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0913 - val_loss: 0.0948\n",
      "Epoch 13/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0895 - val_loss: 0.0923\n",
      "Epoch 14/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0886 - val_loss: 0.0919\n",
      "Epoch 15/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0873 - val_loss: 0.0900\n",
      "Epoch 16/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0865 - val_loss: 0.0891\n",
      "Epoch 17/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0852 - val_loss: 0.0879\n",
      "Epoch 18/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0845 - val_loss: 0.0872\n",
      "Epoch 19/30\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0834 - val_loss: 0.0867\n",
      "Epoch 20/30\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0830 - val_loss: 0.0858\n",
      "Epoch 21/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0820 - val_loss: 0.0846\n",
      "Epoch 22/30\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0812 - val_loss: 0.0849\n",
      "Epoch 23/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0808 - val_loss: 0.0842\n",
      "Epoch 24/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0801 - val_loss: 0.0825\n",
      "Epoch 25/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0797 - val_loss: 0.0836\n",
      "Epoch 26/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0790 - val_loss: 0.0828\n",
      "Epoch 27/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0783 - val_loss: 0.0822\n",
      "Epoch 28/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0777 - val_loss: 0.0824\n",
      "Epoch 29/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0771 - val_loss: 0.0827\n",
      "Epoch 30/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0779 - val_loss: 0.0812\n",
      "118108/118108 [==============================] - 1s 10us/step\n",
      "Fold 1. auc: 0.9346.\n",
      "Fold 3 started at Sun Sep  1 20:44:30 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/30\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.1681 - val_loss: 0.1171\n",
      "Epoch 2/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1208 - val_loss: 0.1116\n",
      "Epoch 3/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1138 - val_loss: 0.1081\n",
      "Epoch 4/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1096 - val_loss: 0.1053\n",
      "Epoch 5/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1061 - val_loss: 0.1032\n",
      "Epoch 6/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1035 - val_loss: 0.1017\n",
      "Epoch 7/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1009 - val_loss: 0.0996\n",
      "Epoch 8/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0992 - val_loss: 0.0982\n",
      "Epoch 9/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0973 - val_loss: 0.0969\n",
      "Epoch 10/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0955 - val_loss: 0.0952\n",
      "Epoch 11/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0945 - val_loss: 0.0942\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0927 - val_loss: 0.0925\n",
      "Epoch 13/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0909 - val_loss: 0.0912\n",
      "Epoch 14/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0897 - val_loss: 0.0900\n",
      "Epoch 15/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0887 - val_loss: 0.0885\n",
      "Epoch 16/30\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0876 - val_loss: 0.0881\n",
      "Epoch 17/30\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0870 - val_loss: 0.0869\n",
      "Epoch 18/30\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0855 - val_loss: 0.0862\n",
      "Epoch 19/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0851 - val_loss: 0.0855\n",
      "Epoch 20/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0838 - val_loss: 0.0842\n",
      "Epoch 21/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0828 - val_loss: 0.0843\n",
      "Epoch 22/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0824 - val_loss: 0.0834\n",
      "Epoch 23/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0821 - val_loss: 0.0829\n",
      "Epoch 24/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0815 - val_loss: 0.0826\n",
      "Epoch 25/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0807 - val_loss: 0.0825\n",
      "Epoch 26/30\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0798 - val_loss: 0.0820\n",
      "Epoch 27/30\n",
      "440000/472432 [==========================>...] - ETA: 1s - loss: 0.0795"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "result_dict_keras = train_model_classification(model=NNModel_maker, \n",
    "                                             X=X,\n",
    "                                             X_test=test,\n",
    "                                             y=y, params=params, folds=folds,\n",
    "                                             model_type=train_options['model_type'], \n",
    "                                             eval_metric=train_options['eval_metric'],\n",
    "                                             plot_feature_importance=True,\n",
    "                                             averaging=train_options['averaging'],\n",
    "                                             groups=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T18:49:20.003271Z",
     "start_time": "2019-08-31T18:49:19.669228Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(f'../../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T18:49:25.000453Z",
     "start_time": "2019-08-31T18:49:22.806366Z"
    }
   },
   "outputs": [],
   "source": [
    "sub['isFraud'] = result_dict_keras['prediction']\n",
    "sub.to_csv('ieee_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T18:49:26.439453Z",
     "start_time": "2019-08-31T18:49:26.180442Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict_keras['prediction'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T17:12:54.017490Z",
     "start_time": "2019-08-31T17:12:53.773517Z"
    }
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-31T18:49:36.738859Z",
     "start_time": "2019-08-31T18:49:36.469799Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{model_folder}/results_dict.pkl', 'wb') as f:\n",
    "#     q = json.dumps(result_dict_lgb,indent=2)\n",
    "    pickle.dump(result_dict_keras,f)\n",
    "#     f.write(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "754px",
    "left": "1526px",
    "right": "20px",
    "top": "96px",
    "width": "344px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
