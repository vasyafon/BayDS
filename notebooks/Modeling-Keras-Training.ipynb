{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:09:34.818294Z",
     "start_time": "2019-09-06T20:09:29.780175Z"
=======
     "end_time": "2019-09-06T21:30:25.507819Z",
     "start_time": "2019-09-06T21:30:21.096822Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
<<<<<<< HEAD
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python37\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
=======
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\python36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from keras.layers import Concatenate, Input, Dense, Embedding, Flatten, Dropout, BatchNormalization, SpatialDropout1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import  Adam\n",
    "import keras.backend as k\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# pd.options.display.precision = 15\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import time\n",
    "# import datetime\n",
    "# from catboost import CatBoostRegressor\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "# from sklearn import metrics\n",
    "# from sklearn import linear_model\n",
    "import gc\n",
    "import pickle\n",
    "# import seaborn as sns\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import eli5\n",
    "# import shap\n",
    "# from IPython.display import HTML\n",
    "# import json\n",
    "# import altair as alt\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "gc.collect()\n",
    "# alt.renderers.enable('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:09:55.244722Z",
     "start_time": "2019-09-06T20:09:55.014716Z"
=======
     "end_time": "2019-09-06T21:30:25.664824Z",
     "start_time": "2019-09-06T21:30:25.509821Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "main_path = r'../..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:09:57.018765Z",
     "start_time": "2019-09-06T20:09:56.187734Z"
=======
     "end_time": "2019-09-06T21:30:26.864834Z",
     "start_time": "2019-09-06T21:30:25.666820Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(main_path)\n",
    "from BayDS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:09:58.576779Z",
     "start_time": "2019-09-06T20:09:58.319771Z"
=======
     "end_time": "2019-09-06T21:30:27.019821Z",
     "start_time": "2019-09-06T21:30:26.865819Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = '31.08'\n",
    "main_learning_folder = main_path+'/Snapshots/'+experiment_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:39.354469Z",
     "start_time": "2019-09-06T20:10:25.484238Z"
=======
     "end_time": "2019-09-06T21:30:44.540086Z",
     "start_time": "2019-09-06T21:30:27.020820Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "35"
=======
       "11"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start here\n",
    "data_folder = main_path+'/Data'\n",
    "y = pd.read_pickle(f'{data_folder}/y.pkl')\n",
    "X = pd.read_pickle(f'{data_folder}/X_encoded_scaled.pkl').astype(np.float32)\n",
    "test = pd.read_pickle(f'{data_folder}/test_encoded_scaled.pkl').astype(np.float32)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:42.845525Z",
     "start_time": "2019-09-06T20:10:42.580518Z"
=======
     "end_time": "2019-09-07T06:19:10.787215Z",
     "start_time": "2019-09-07T06:19:10.165434Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "# Setting model_folder\n",
    "model_name = 'keras-4Layer'\n",
    "model_folder = f'{main_learning_folder}/{model_name}'\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:43.998545Z",
     "start_time": "2019-09-06T20:10:43.737539Z"
=======
     "end_time": "2019-09-06T21:30:44.913100Z",
     "start_time": "2019-09-06T21:30:44.760086Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "# folds = TimeSeriesSplit(n_splits=n_fold)\n",
    "folds = KFold(n_splits=n_fold)\n",
    "# folds = GroupKFold(n_splits=5)\n",
    "# groups = pd.read_pickle('./groups.pkl').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:45.075587Z",
     "start_time": "2019-09-06T20:10:44.802558Z"
=======
     "end_time": "2019-09-06T21:30:45.095090Z",
     "start_time": "2019-09-06T21:30:44.914086Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    569877\n",
       "1     20663\n",
       "Name: isFraud, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:46.129614Z",
     "start_time": "2019-09-06T20:10:45.860576Z"
=======
     "end_time": "2019-09-07T06:55:54.305544Z",
     "start_time": "2019-09-07T06:55:54.149543Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "from keras.regularizers import l1,l2\n",
    "\n",
    "def NNModel_maker():\n",
    "    k.clear_session()\n",
    "    \n",
    "#     categorical_inputs = []\n",
    "#     for cat in categorical:\n",
    "#         categorical_inputs.append(Input(shape=[1], name=cat))\n",
    "\n",
    "#     categorical_embeddings = []\n",
    "#     for i, cat in enumerate(categorical):\n",
    "#         categorical_embeddings.append(\n",
    "#             Embedding(category_counts[cat], int(np.log1p(category_counts[cat]) + 1), name = cat + \\\n",
    "#                       \"_embed\")(categorical_inputs[i]))\n",
    "\n",
    "#     categorical_logits = Concatenate(name = \"categorical_conc\")([Flatten()(SpatialDropout1D(.1)(cat_emb)) for cat_emb in categorical_embeddings])\n",
    "# \n",
    "    numerical_inputs = Input(shape=[X.shape[1]], name = 'all')\n",
<<<<<<< HEAD
    "    numerical_logits = Dropout(.3)(numerical_inputs)\n",
    "  \n",
    "    x = numerical_logits\n",
    "    x = Dense(400, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(400, activation = 'relu')(x)\n",
=======
    "    x = numerical_inputs\n",
    "    x = Dense(400, activation = 'relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(400, activation = 'relu', kernel_regularizer=l2(0.001))(x)\n",
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(200, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = Dense(100, activation = 'relu')(x)\n",
    "    x = Dropout(.3)(x)\n",
    "    x = BatchNormalization()(x)     \n",
    "    out = Dense(1, activation = 'sigmoid')(x)    \n",
    "    model = Model(inputs= [numerical_inputs],outputs=out)\n",
    "    loss = \"binary_crossentropy\"\n",
    "    model.compile(optimizer=Adam(lr = 0.0003), loss = loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "params = {\n",
    "    'batch_size': 1024,\n",
    "    'epochs': 100,\n",
    "    'verbose': True,\n",
    "         }\n",
    "train_options = {\n",
    "    \"model_type\":'keras',\n",
    "    \"params\": params,\n",
    "    \"eval_metric\":'auc',\n",
    "    'averaging': 'usual',\n",
    "    'use_groups': False,\n",
    "    'fold_name': folds.__class__.__name__,\n",
    "    'n_splits': n_fold\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:50.666653Z",
     "start_time": "2019-09-06T20:10:50.378678Z"
=======
     "end_time": "2019-09-07T06:55:46.167255Z",
     "start_time": "2019-09-07T06:55:46.014245Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{model_folder}/training_params.json', 'w') as f:\n",
    "    q = json.dumps(train_options,indent=2)\n",
    "    f.write(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-09-06T20:10:58.992828Z",
     "start_time": "2019-09-06T20:10:52.643723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
=======
     "end_time": "2019-09-07T06:56:00.190591Z",
     "start_time": "2019-09-07T06:55:58.668787Z"
    }
   },
   "outputs": [],
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
   "source": [
    "NNModel_maker().save(f'{model_folder}/keras.mdl')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T20:11:00.148828Z",
     "start_time": "2019-09-06T20:10:59.787806Z"
=======
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:19:45.936140Z",
     "start_time": "2019-09-07T06:19:45.740140Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
<<<<<<< HEAD
       " incarnation: 9394828649611977625, name: \"/device:GPU:0\"\n",
=======
       " incarnation: 2295671409795812128, name: \"/device:GPU:0\"\n",
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
       " device_type: \"GPU\"\n",
       " memory_limit: 1452988825\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
<<<<<<< HEAD
       " incarnation: 4512666971317458215\n",
       " physical_device_desc: \"device: 0, name: Quadro M1000M, pci bus id: 0000:01:00.0, compute capability: 5.0\"]"
      ]
     },
     "execution_count": 16,
=======
       " incarnation: 943391395817768365\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:07:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 25,
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T20:11:01.225846Z",
     "start_time": "2019-09-06T20:11:00.971823Z"
=======
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T06:19:48.676283Z",
     "start_time": "2019-09-07T06:19:48.524282Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 17,
=======
     "execution_count": 26,
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T20:11:04.860892Z",
     "start_time": "2019-09-06T20:11:04.505889Z"
=======
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:30:48.727542Z",
     "start_time": "2019-09-06T21:30:48.529971Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T20:11:17.838140Z",
     "start_time": "2019-09-06T20:11:17.479104Z"
=======
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:30:48.918816Z",
     "start_time": "2019-09-06T21:30:48.728538Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU':8},log_device_placement=True) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:28.215022Z",
     "start_time": "2019-09-06T20:11:19.374135Z"
=======
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T10:11:42.699873Z",
     "start_time": "2019-09-07T06:56:02.669274Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Fold 1 started at Fri Sep  6 23:11:20 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.6667 - val_loss: 0.8111\n",
      "Epoch 2/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.4309 - val_loss: 0.5337\n",
      "Epoch 3/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.2878 - val_loss: 0.4075\n",
      "Epoch 4/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.2118 - val_loss: 0.3171\n",
      "Epoch 5/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1689 - val_loss: 0.2644\n",
      "Epoch 6/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1469 - val_loss: 0.2036\n",
      "Epoch 7/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1335 - val_loss: 0.1681\n",
      "Epoch 8/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1249 - val_loss: 0.1480\n",
      "Epoch 9/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1176 - val_loss: 0.1114\n",
      "Epoch 10/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1127 - val_loss: 0.1074\n",
      "Epoch 11/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1092 - val_loss: 0.0963\n",
      "Epoch 12/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1042 - val_loss: 0.0881\n",
      "Epoch 13/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1014 - val_loss: 0.0795\n",
      "Epoch 14/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0988 - val_loss: 0.0792\n",
      "Epoch 15/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0959 - val_loss: 0.0790\n",
      "Epoch 16/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0944 - val_loss: 0.0784\n",
      "Epoch 17/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0916 - val_loss: 0.0798\n",
      "Epoch 18/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0907 - val_loss: 0.0726\n",
      "Epoch 19/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0897 - val_loss: 0.0751\n",
      "Epoch 20/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0875 - val_loss: 0.0703\n",
      "Epoch 21/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0865 - val_loss: 0.0750\n",
      "Epoch 22/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0854 - val_loss: 0.0731\n",
      "Epoch 23/60\n",
      "472432/472432 [==============================] - 20s 41us/step - loss: 0.0842 - val_loss: 0.0700\n",
      "Epoch 24/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0840 - val_loss: 0.0711\n",
      "Epoch 25/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0831 - val_loss: 0.0730\n",
      "Epoch 26/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0822 - val_loss: 0.0742\n",
      "Epoch 27/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0822 - val_loss: 0.0697\n",
      "Epoch 28/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0808 - val_loss: 0.0682\n",
      "Epoch 29/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0800 - val_loss: 0.0704\n",
      "Epoch 30/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0794 - val_loss: 0.0684\n",
      "Epoch 31/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0792 - val_loss: 0.0726\n",
      "Epoch 32/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0782 - val_loss: 0.0675\n",
      "Epoch 33/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0775 - val_loss: 0.0666\n",
      "Epoch 34/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0766 - val_loss: 0.0682\n",
      "Epoch 35/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0764 - val_loss: 0.0695\n",
      "Epoch 36/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0761 - val_loss: 0.0653\n",
      "Epoch 37/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0749 - val_loss: 0.0671\n",
      "Epoch 38/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0748 - val_loss: 0.0679\n",
      "Epoch 39/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0751 - val_loss: 0.0660\n",
      "Epoch 40/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0748 - val_loss: 0.0660\n",
      "Epoch 41/60\n",
      "472432/472432 [==============================] - 23s 50us/step - loss: 0.0745 - val_loss: 0.0645\n",
      "Epoch 42/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0731 - val_loss: 0.0654\n",
      "Epoch 43/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0729 - val_loss: 0.0664\n",
      "Epoch 44/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0724 - val_loss: 0.0636\n",
      "Epoch 45/60\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0722 - val_loss: 0.0667\n",
      "Epoch 46/60\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0714 - val_loss: 0.0642\n",
      "Epoch 47/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0716 - val_loss: 0.0635\n",
      "Epoch 48/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0712 - val_loss: 0.0641\n",
      "Epoch 49/60\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0711 - val_loss: 0.0651\n",
      "Epoch 50/60\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0701 - val_loss: 0.0631\n",
      "Epoch 51/60\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0705 - val_loss: 0.0661\n",
      "Epoch 52/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0697 - val_loss: 0.0638\n",
      "Epoch 53/60\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0692 - val_loss: 0.0622\n",
      "Epoch 54/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0688 - val_loss: 0.0648\n",
      "Epoch 55/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0689 - val_loss: 0.0620\n",
      "Epoch 56/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0689 - val_loss: 0.0656\n",
      "Epoch 57/60\n",
      "472432/472432 [==============================] - 17s 37us/step - loss: 0.0678 - val_loss: 0.0627\n",
      "Epoch 58/60\n",
      "472432/472432 [==============================] - 18s 37us/step - loss: 0.0673 - val_loss: 0.0624\n",
      "Epoch 59/60\n",
      "472432/472432 [==============================] - 18s 37us/step - loss: 0.0672 - val_loss: 0.0614\n",
      "Epoch 60/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0671 - val_loss: 0.0627\n",
      "118108/118108 [==============================] - 1s 7us/step\n",
      "Fold 0. auc: 0.9290.\n",
      "Fold 2 started at Fri Sep  6 23:31:06 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.6824 - val_loss: 0.5246\n",
      "Epoch 2/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.4641 - val_loss: 0.3973\n",
      "Epoch 3/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.3044 - val_loss: 0.2762\n",
      "Epoch 4/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.2184 - val_loss: 0.2211\n",
      "Epoch 5/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1708 - val_loss: 0.1755\n",
      "Epoch 6/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.1437 - val_loss: 0.1500\n",
      "Epoch 7/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1289 - val_loss: 0.1317\n",
      "Epoch 8/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1184 - val_loss: 0.1229\n",
      "Epoch 9/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1120 - val_loss: 0.1161\n",
      "Epoch 10/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1066 - val_loss: 0.1097\n",
      "Epoch 11/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1026 - val_loss: 0.1081\n",
      "Epoch 12/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0983 - val_loss: 0.1024\n",
      "Epoch 13/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0957 - val_loss: 0.1012\n"
=======
      "Fold 1 started at Sat Sep  7 09:56:03 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.9473 - val_loss: 0.5850\n",
      "Epoch 2/100\n",
      "472432/472432 [==============================] - 23s 49us/step - loss: 0.3459 - val_loss: 0.2370\n",
      "Epoch 3/100\n",
      "472432/472432 [==============================] - 24s 52us/step - loss: 0.1950 - val_loss: 0.1525\n",
      "Epoch 4/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.1387 - val_loss: 0.1360\n",
      "Epoch 5/100\n",
      "472432/472432 [==============================] - 24s 50us/step - loss: 0.1155 - val_loss: 0.0943\n",
      "Epoch 6/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.1024 - val_loss: 0.0869\n",
      "Epoch 7/100\n",
      "472432/472432 [==============================] - 24s 50us/step - loss: 0.0961 - val_loss: 0.0926\n",
      "Epoch 8/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0925 - val_loss: 0.0866\n",
      "Epoch 9/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0900 - val_loss: 0.0871\n",
      "Epoch 10/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0873 - val_loss: 0.0810\n",
      "Epoch 11/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0868 - val_loss: 0.0781\n",
      "Epoch 12/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0857 - val_loss: 0.0781\n",
      "Epoch 13/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0845 - val_loss: 0.0893\n",
      "Epoch 14/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0834 - val_loss: 0.0846\n",
      "Epoch 15/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0829 - val_loss: 0.0808\n",
      "Epoch 16/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0823 - val_loss: 0.0746\n",
      "Epoch 17/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0819 - val_loss: 0.0820\n",
      "Epoch 18/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0814 - val_loss: 0.0793\n",
      "Epoch 19/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0809 - val_loss: 0.0747\n",
      "Epoch 20/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0806 - val_loss: 0.0787\n",
      "Epoch 21/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0803 - val_loss: 0.0797\n",
      "Epoch 22/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0796 - val_loss: 0.0799\n",
      "Epoch 23/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0792 - val_loss: 0.0776\n",
      "Epoch 24/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0790 - val_loss: 0.0756\n",
      "Epoch 25/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0787 - val_loss: 0.0787\n",
      "Epoch 26/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0783 - val_loss: 0.0765\n",
      "Epoch 27/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0785 - val_loss: 0.0747\n",
      "Epoch 28/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0787 - val_loss: 0.0750\n",
      "Epoch 29/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0781 - val_loss: 0.0751\n",
      "Epoch 30/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0784 - val_loss: 0.0763\n",
      "Epoch 31/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0776 - val_loss: 0.0779\n",
      "Epoch 32/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0780 - val_loss: 0.0769\n",
      "Epoch 33/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0775 - val_loss: 0.0794\n",
      "Epoch 34/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0777 - val_loss: 0.0734\n",
      "Epoch 35/100\n",
      "472432/472432 [==============================] - 24s 50us/step - loss: 0.0773 - val_loss: 0.0812\n",
      "Epoch 36/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0774 - val_loss: 0.0774\n",
      "Epoch 37/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0774 - val_loss: 0.0761\n",
      "Epoch 38/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0766 - val_loss: 0.0786\n",
      "Epoch 39/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0766 - val_loss: 0.0832\n",
      "Epoch 40/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0767 - val_loss: 0.0868\n",
      "Epoch 41/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0770 - val_loss: 0.0755\n",
      "Epoch 42/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0767 - val_loss: 0.0766\n",
      "Epoch 43/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0764 - val_loss: 0.0787\n",
      "Epoch 44/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0762 - val_loss: 0.0768\n",
      "Epoch 45/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0761 - val_loss: 0.0778\n",
      "Epoch 46/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0760 - val_loss: 0.0777\n",
      "Epoch 47/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0764 - val_loss: 0.0775\n",
      "Epoch 48/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0758 - val_loss: 0.0786\n",
      "Epoch 49/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0760 - val_loss: 0.0821\n",
      "Epoch 50/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0761 - val_loss: 0.0769\n",
      "Epoch 51/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0755 - val_loss: 0.0761\n",
      "Epoch 52/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0760 - val_loss: 0.0772\n",
      "Epoch 53/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0756 - val_loss: 0.0785\n",
      "Epoch 54/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0757 - val_loss: 0.0771\n",
      "Epoch 55/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0758 - val_loss: 0.0773\n",
      "Epoch 56/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0755 - val_loss: 0.0799\n",
      "Epoch 57/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0756 - val_loss: 0.0753\n",
      "Epoch 58/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0756 - val_loss: 0.0756\n",
      "Epoch 59/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0750 - val_loss: 0.0743\n",
      "Epoch 60/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0746 - val_loss: 0.0724\n",
      "Epoch 61/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0759 - val_loss: 0.0770\n",
      "Epoch 62/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0751 - val_loss: 0.0752\n",
      "Epoch 63/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0754 - val_loss: 0.0748\n",
      "Epoch 64/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0748 - val_loss: 0.0715\n",
      "Epoch 65/100\n",
      "472432/472432 [==============================] - 22s 48us/step - loss: 0.0754 - val_loss: 0.0744\n",
      "Epoch 66/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0752 - val_loss: 0.0745\n",
      "Epoch 67/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0751 - val_loss: 0.0753\n",
      "Epoch 68/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0752 - val_loss: 0.0862\n",
      "Epoch 69/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0748 - val_loss: 0.0779\n",
      "Epoch 70/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0746 - val_loss: 0.0790\n",
      "Epoch 71/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0743 - val_loss: 0.0790\n",
      "Epoch 72/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0747 - val_loss: 0.0777\n",
      "Epoch 73/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0749 - val_loss: 0.0758\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0745 - val_loss: 0.0775\n",
      "Epoch 75/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0746 - val_loss: 0.0800\n",
      "Epoch 76/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0774\n",
      "Epoch 77/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0749 - val_loss: 0.0776\n",
      "Epoch 78/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0774\n",
      "Epoch 79/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0765\n",
      "Epoch 80/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0745 - val_loss: 0.0763\n",
      "Epoch 81/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0745 - val_loss: 0.0780\n",
      "Epoch 82/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0747 - val_loss: 0.0800\n",
      "Epoch 83/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0744 - val_loss: 0.0804\n",
      "Epoch 84/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0739\n",
      "Epoch 85/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0748 - val_loss: 0.0744\n",
      "Epoch 86/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0742 - val_loss: 0.0787\n",
      "Epoch 87/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0745 - val_loss: 0.0858\n",
      "Epoch 88/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0775\n",
      "Epoch 89/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0743 - val_loss: 0.0754\n",
      "Epoch 90/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0746 - val_loss: 0.0761\n",
      "Epoch 91/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0745 - val_loss: 0.0753\n",
      "Epoch 92/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0745 - val_loss: 0.0773\n",
      "Epoch 93/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0746 - val_loss: 0.0780\n",
      "Epoch 94/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0740 - val_loss: 0.0804\n",
      "Epoch 95/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0744 - val_loss: 0.0740\n",
      "Epoch 96/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0741 - val_loss: 0.0739\n",
      "Epoch 97/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0743 - val_loss: 0.0760\n",
      "Epoch 98/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0746 - val_loss: 0.0730\n",
      "Epoch 99/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0741 - val_loss: 0.0800\n",
      "Epoch 100/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0743 - val_loss: 0.0772\n",
      "118108/118108 [==============================] - 1s 12us/step\n",
      "Fold 0. auc: 0.9205.\n",
      "Fold 2 started at Sat Sep  7 10:32:31 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.9666 - val_loss: 0.5096\n",
      "Epoch 2/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.3560 - val_loss: 0.2642\n",
      "Epoch 3/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.2015 - val_loss: 0.1690\n",
      "Epoch 4/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1419 - val_loss: 0.1364\n",
      "Epoch 5/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1139 - val_loss: 0.1170\n",
      "Epoch 6/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0994 - val_loss: 0.1082\n",
      "Epoch 7/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0928 - val_loss: 0.1070\n",
      "Epoch 8/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0880 - val_loss: 0.0997\n",
      "Epoch 9/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0849 - val_loss: 0.0947\n",
      "Epoch 10/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0838 - val_loss: 0.0955\n",
      "Epoch 11/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0817 - val_loss: 0.0964\n",
      "Epoch 12/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0812 - val_loss: 0.0977\n",
      "Epoch 13/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0800 - val_loss: 0.0982\n",
      "Epoch 14/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0791 - val_loss: 0.0962\n",
      "Epoch 15/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0787 - val_loss: 0.0918\n",
      "Epoch 16/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0782 - val_loss: 0.0934\n",
      "Epoch 17/100\n",
      "472432/472432 [==============================] - 23s 49us/step - loss: 0.0778 - val_loss: 0.0953\n",
      "Epoch 18/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0775 - val_loss: 0.0939\n",
      "Epoch 19/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0768 - val_loss: 0.0914\n",
      "Epoch 20/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0760 - val_loss: 0.0930\n",
      "Epoch 21/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0758 - val_loss: 0.0921\n",
      "Epoch 22/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0753 - val_loss: 0.0974\n",
      "Epoch 23/100\n",
      "472432/472432 [==============================] - 24s 51us/step - loss: 0.0760 - val_loss: 0.0934\n",
      "Epoch 24/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0749 - val_loss: 0.0937\n",
      "Epoch 25/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0750 - val_loss: 0.0919\n",
      "Epoch 26/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0748 - val_loss: 0.0955\n",
      "Epoch 27/100\n",
      "472432/472432 [==============================] - 25s 54us/step - loss: 0.0743 - val_loss: 0.0945\n",
      "Epoch 28/100\n",
      "472432/472432 [==============================] - 24s 51us/step - loss: 0.0741 - val_loss: 0.0912\n",
      "Epoch 29/100\n",
      "472432/472432 [==============================] - 24s 52us/step - loss: 0.0740 - val_loss: 0.0902\n",
      "Epoch 30/100\n",
      "472432/472432 [==============================] - 22s 48us/step - loss: 0.0742 - val_loss: 0.0891\n",
      "Epoch 31/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0739 - val_loss: 0.0930\n",
      "Epoch 32/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0737 - val_loss: 0.0941\n",
      "Epoch 33/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0732 - val_loss: 0.0928\n",
      "Epoch 34/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0733 - val_loss: 0.0928\n",
      "Epoch 35/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0737 - val_loss: 0.0919\n",
      "Epoch 36/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0726 - val_loss: 0.0919\n",
      "Epoch 37/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0729 - val_loss: 0.0915\n",
      "Epoch 38/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0729 - val_loss: 0.0890\n",
      "Epoch 39/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0724 - val_loss: 0.0918\n",
      "Epoch 40/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0724 - val_loss: 0.0925\n",
      "Epoch 41/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0726 - val_loss: 0.0946\n",
      "Epoch 42/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0728 - val_loss: 0.0947\n",
      "Epoch 43/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0726 - val_loss: 0.0946\n",
      "Epoch 44/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0719 - val_loss: 0.0923\n",
      "Epoch 45/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0721 - val_loss: 0.0970\n",
      "Epoch 46/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0725 - val_loss: 0.0920\n",
      "Epoch 47/100\n"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 14/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0930 - val_loss: 0.0963\n",
      "Epoch 15/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0903 - val_loss: 0.0948\n",
      "Epoch 16/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0878 - val_loss: 0.0904\n",
      "Epoch 17/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0866 - val_loss: 0.0916\n",
      "Epoch 18/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0848 - val_loss: 0.0909\n",
      "Epoch 19/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0833 - val_loss: 0.0897\n",
      "Epoch 20/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0820 - val_loss: 0.0900\n",
      "Epoch 21/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0810 - val_loss: 0.0869\n",
      "Epoch 22/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0796 - val_loss: 0.0869\n",
      "Epoch 23/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0788 - val_loss: 0.0860\n",
      "Epoch 24/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0780 - val_loss: 0.0853\n",
      "Epoch 25/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0775 - val_loss: 0.0840\n",
      "Epoch 26/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0767 - val_loss: 0.0834\n",
      "Epoch 27/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0764 - val_loss: 0.0850\n",
      "Epoch 28/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0756 - val_loss: 0.0827\n",
      "Epoch 29/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0748 - val_loss: 0.0826\n",
      "Epoch 30/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0741 - val_loss: 0.0821\n",
      "Epoch 31/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0737 - val_loss: 0.0821\n",
      "Epoch 32/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0732 - val_loss: 0.0821\n",
      "Epoch 33/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0724 - val_loss: 0.0812\n",
      "Epoch 34/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0726 - val_loss: 0.0798\n",
      "Epoch 35/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0720 - val_loss: 0.0806\n",
      "Epoch 36/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0711 - val_loss: 0.0819\n",
      "Epoch 37/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0707 - val_loss: 0.0805\n",
      "Epoch 38/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0707 - val_loss: 0.0806\n",
      "Epoch 39/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0702 - val_loss: 0.0808\n",
      "Epoch 40/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0692 - val_loss: 0.0786\n",
      "Epoch 41/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0690 - val_loss: 0.0802\n",
      "Epoch 42/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0686 - val_loss: 0.0806\n",
      "Epoch 43/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0682 - val_loss: 0.0797\n",
      "Epoch 44/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0678 - val_loss: 0.0804\n",
      "Epoch 45/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0677 - val_loss: 0.0788\n",
      "Epoch 46/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0679 - val_loss: 0.0795\n",
      "Epoch 47/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0671 - val_loss: 0.0774\n",
      "Epoch 48/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0666 - val_loss: 0.0783\n",
      "Epoch 49/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0664 - val_loss: 0.0795\n",
      "Epoch 50/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0659 - val_loss: 0.0790\n",
      "Epoch 51/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0652 - val_loss: 0.0787\n",
      "Epoch 52/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0657 - val_loss: 0.0780\n",
      "Epoch 53/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0651 - val_loss: 0.0777\n",
      "Epoch 54/60\n",
      "472432/472432 [==============================] - 20s 41us/step - loss: 0.0646 - val_loss: 0.0777\n",
      "Epoch 55/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0645 - val_loss: 0.0777\n",
      "Epoch 56/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0642 - val_loss: 0.0775\n",
      "Epoch 57/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0640 - val_loss: 0.0790\n",
      "Epoch 58/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0637 - val_loss: 0.0778\n",
      "Epoch 59/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0631 - val_loss: 0.0784\n",
      "Epoch 60/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0634 - val_loss: 0.0777\n",
      "118108/118108 [==============================] - 1s 7us/step\n",
      "Fold 1. auc: 0.9433.\n",
      "Fold 3 started at Fri Sep  6 23:49:54 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.6893 - val_loss: 0.5761\n",
      "Epoch 2/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.4754 - val_loss: 0.4109\n",
      "Epoch 3/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.3052 - val_loss: 0.2812\n",
      "Epoch 4/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.2157 - val_loss: 0.2081\n",
      "Epoch 5/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1691 - val_loss: 0.1644\n",
      "Epoch 6/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1429 - val_loss: 0.1415\n",
      "Epoch 7/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1280 - val_loss: 0.1269\n",
      "Epoch 8/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1192 - val_loss: 0.1167\n",
      "Epoch 9/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1122 - val_loss: 0.1110\n",
      "Epoch 10/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1075 - val_loss: 0.1061\n",
      "Epoch 11/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1029 - val_loss: 0.1026\n",
      "Epoch 12/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0997 - val_loss: 0.0991\n",
      "Epoch 13/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0964 - val_loss: 0.0964\n",
      "Epoch 14/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0934 - val_loss: 0.0964\n",
      "Epoch 15/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0909 - val_loss: 0.0937\n",
      "Epoch 16/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0884 - val_loss: 0.0894\n",
      "Epoch 17/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0873 - val_loss: 0.0898\n",
      "Epoch 18/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0858 - val_loss: 0.0876\n",
      "Epoch 19/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0846 - val_loss: 0.0888\n",
      "Epoch 20/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0841 - val_loss: 0.0863\n",
      "Epoch 21/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0820 - val_loss: 0.0866\n",
      "Epoch 22/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0812 - val_loss: 0.0854\n",
      "Epoch 23/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0803 - val_loss: 0.0852\n",
      "Epoch 24/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0797 - val_loss: 0.0835\n",
      "Epoch 25/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0782 - val_loss: 0.0817\n",
      "Epoch 26/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0780 - val_loss: 0.0836\n",
      "Epoch 27/60\n"
=======
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0724 - val_loss: 0.0938\n",
      "Epoch 48/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0721 - val_loss: 0.0962\n",
      "Epoch 49/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0716 - val_loss: 0.0907\n",
      "Epoch 50/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0717 - val_loss: 0.0915\n",
      "Epoch 51/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0721 - val_loss: 0.0907\n",
      "Epoch 52/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0718 - val_loss: 0.0916\n",
      "Epoch 53/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0714 - val_loss: 0.0901\n",
      "Epoch 54/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0720 - val_loss: 0.0924\n",
      "Epoch 55/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0712 - val_loss: 0.0936\n",
      "Epoch 56/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0716 - val_loss: 0.0943\n",
      "Epoch 57/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0717 - val_loss: 0.0900\n",
      "Epoch 58/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0717 - val_loss: 0.0913\n",
      "Epoch 59/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0717 - val_loss: 0.0921\n",
      "Epoch 60/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0715 - val_loss: 0.0928\n",
      "Epoch 61/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0715 - val_loss: 0.0921\n",
      "Epoch 62/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0711 - val_loss: 0.0915\n",
      "Epoch 63/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0710 - val_loss: 0.0931\n",
      "Epoch 64/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0709 - val_loss: 0.0904\n",
      "Epoch 65/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0712 - val_loss: 0.0912\n",
      "Epoch 66/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0714 - val_loss: 0.0918\n",
      "Epoch 67/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0709 - val_loss: 0.0894\n",
      "Epoch 68/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0711 - val_loss: 0.0942\n",
      "Epoch 69/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0709 - val_loss: 0.0907\n",
      "Epoch 70/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0707 - val_loss: 0.0920\n",
      "Epoch 71/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0707 - val_loss: 0.0967\n",
      "Epoch 72/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0707 - val_loss: 0.0916\n",
      "Epoch 73/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0706 - val_loss: 0.0911\n",
      "Epoch 74/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0704 - val_loss: 0.0930\n",
      "Epoch 75/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0708 - val_loss: 0.0923\n",
      "Epoch 76/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0705 - val_loss: 0.0961\n",
      "Epoch 77/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0705 - val_loss: 0.0945\n",
      "Epoch 78/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0703 - val_loss: 0.0913\n",
      "Epoch 79/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0709 - val_loss: 0.0911\n",
      "Epoch 80/100\n",
      "472432/472432 [==============================] - 30s 64us/step - loss: 0.0707 - val_loss: 0.0923\n",
      "Epoch 81/100\n",
      "472432/472432 [==============================] - 29s 62us/step - loss: 0.0703 - val_loss: 0.0983\n",
      "Epoch 82/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0708 - val_loss: 0.0937\n",
      "Epoch 83/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0704 - val_loss: 0.0918\n",
      "Epoch 84/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0703 - val_loss: 0.0895\n",
      "Epoch 85/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0708 - val_loss: 0.0886\n",
      "Epoch 86/100\n",
      "472432/472432 [==============================] - 25s 54us/step - loss: 0.0701 - val_loss: 0.0909\n",
      "Epoch 87/100\n",
      "472432/472432 [==============================] - 25s 52us/step - loss: 0.0703 - val_loss: 0.0904\n",
      "Epoch 88/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0702 - val_loss: 0.0920\n",
      "Epoch 89/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0699 - val_loss: 0.0937\n",
      "Epoch 90/100\n",
      "472432/472432 [==============================] - 25s 52us/step - loss: 0.0700 - val_loss: 0.0920\n",
      "Epoch 91/100\n",
      "472432/472432 [==============================] - 25s 52us/step - loss: 0.0702 - val_loss: 0.0919\n",
      "Epoch 92/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0703 - val_loss: 0.0912\n",
      "Epoch 93/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0697 - val_loss: 0.0929\n",
      "Epoch 94/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0713 - val_loss: 0.0936\n",
      "Epoch 95/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0704 - val_loss: 0.0946\n",
      "Epoch 96/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0698 - val_loss: 0.0934\n",
      "Epoch 97/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0697 - val_loss: 0.0900\n",
      "Epoch 98/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0702 - val_loss: 0.0908\n",
      "Epoch 99/100\n",
      "472432/472432 [==============================] - 25s 54us/step - loss: 0.0707 - val_loss: 0.0892\n",
      "Epoch 100/100\n",
      "472432/472432 [==============================] - 29s 60us/step - loss: 0.0700 - val_loss: 0.0943\n",
      "118108/118108 [==============================] - 1s 12us/step\n",
      "Fold 1. auc: 0.9400.\n",
      "Fold 3 started at Sat Sep  7 11:10:03 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/100\n",
      "472432/472432 [==============================] - 24s 50us/step - loss: 0.9760 - val_loss: 0.5110\n",
      "Epoch 2/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.3538 - val_loss: 0.2543\n",
      "Epoch 3/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.1998 - val_loss: 0.1679\n",
      "Epoch 4/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.1384 - val_loss: 0.1288\n",
      "Epoch 5/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.1117 - val_loss: 0.1125\n",
      "Epoch 6/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0985 - val_loss: 0.1018\n",
      "Epoch 7/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0915 - val_loss: 0.0994\n",
      "Epoch 8/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0875 - val_loss: 0.0967\n",
      "Epoch 9/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0850 - val_loss: 0.0980\n",
      "Epoch 10/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0839 - val_loss: 0.0959\n",
      "Epoch 11/100\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0822 - val_loss: 0.0962\n",
      "Epoch 12/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0804 - val_loss: 0.1037\n",
      "Epoch 13/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0801 - val_loss: 0.0933\n",
      "Epoch 14/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0789 - val_loss: 0.0944\n",
      "Epoch 15/100\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0787 - val_loss: 0.0917\n",
      "Epoch 16/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0778 - val_loss: 0.0960\n",
      "Epoch 17/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0773 - val_loss: 0.0946\n",
      "Epoch 18/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0768 - val_loss: 0.0912\n",
      "Epoch 19/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0761 - val_loss: 0.0918\n",
      "Epoch 20/100\n"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0768 - val_loss: 0.0825\n",
      "Epoch 28/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0766 - val_loss: 0.0809\n",
      "Epoch 29/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0760 - val_loss: 0.0818\n",
      "Epoch 30/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0747 - val_loss: 0.0820\n",
      "Epoch 31/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0745 - val_loss: 0.0800\n",
      "Epoch 32/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0741 - val_loss: 0.0819\n",
      "Epoch 33/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0741 - val_loss: 0.0790\n",
      "Epoch 34/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0730 - val_loss: 0.0805\n",
      "Epoch 35/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0732 - val_loss: 0.0804\n",
      "Epoch 36/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0719 - val_loss: 0.0785\n",
      "Epoch 37/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0712 - val_loss: 0.0802\n",
      "Epoch 38/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0712 - val_loss: 0.0785\n",
      "Epoch 39/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0715 - val_loss: 0.0791\n",
      "Epoch 40/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0707 - val_loss: 0.0787\n",
      "Epoch 41/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0701 - val_loss: 0.0789\n",
      "Epoch 42/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0696 - val_loss: 0.0773\n",
      "Epoch 43/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0692 - val_loss: 0.0768\n",
      "Epoch 44/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0690 - val_loss: 0.0769\n",
      "Epoch 45/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0684 - val_loss: 0.0770\n",
      "Epoch 46/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0680 - val_loss: 0.0770\n",
      "Epoch 47/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0681 - val_loss: 0.0783\n",
      "Epoch 48/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0675 - val_loss: 0.0770\n",
      "Epoch 49/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0672 - val_loss: 0.0763\n",
      "Epoch 50/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0671 - val_loss: 0.0770\n",
      "Epoch 51/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0669 - val_loss: 0.0769\n",
      "Epoch 52/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0668 - val_loss: 0.0757\n",
      "Epoch 53/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0665 - val_loss: 0.0771\n",
      "Epoch 54/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0660 - val_loss: 0.0768\n",
      "Epoch 55/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0649 - val_loss: 0.0756\n",
      "Epoch 56/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0652 - val_loss: 0.0760\n",
      "Epoch 57/60\n",
      "472432/472432 [==============================] - 20s 43us/step - loss: 0.0648 - val_loss: 0.0758\n",
      "Epoch 58/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0647 - val_loss: 0.0756\n",
      "Epoch 59/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0647 - val_loss: 0.0761\n",
      "Epoch 60/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0640 - val_loss: 0.0749\n",
      "118108/118108 [==============================] - 1s 7us/step\n",
      "Fold 2. auc: 0.9388.\n",
      "Fold 4 started at Sat Sep  7 00:09:31 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.6807 - val_loss: 0.5270\n",
      "Epoch 2/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.4693 - val_loss: 0.3625\n",
      "Epoch 3/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.3036 - val_loss: 0.2652\n",
      "Epoch 4/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.2134 - val_loss: 0.2106\n",
      "Epoch 5/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1675 - val_loss: 0.1711\n",
      "Epoch 6/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1411 - val_loss: 0.1419\n",
      "Epoch 7/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1261 - val_loss: 0.1298\n",
      "Epoch 8/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1171 - val_loss: 0.1207\n",
      "Epoch 9/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1109 - val_loss: 0.1158\n",
      "Epoch 10/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1056 - val_loss: 0.1111\n",
      "Epoch 11/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.1015 - val_loss: 0.1069\n",
      "Epoch 12/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0974 - val_loss: 0.1058\n",
      "Epoch 13/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0950 - val_loss: 0.1010\n",
      "Epoch 14/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0922 - val_loss: 0.0993\n",
      "Epoch 15/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0899 - val_loss: 0.0988\n",
      "Epoch 16/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0880 - val_loss: 0.0974\n",
      "Epoch 17/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0860 - val_loss: 0.0941\n",
      "Epoch 18/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0847 - val_loss: 0.0926\n",
      "Epoch 19/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0834 - val_loss: 0.0930\n",
      "Epoch 20/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0823 - val_loss: 0.0913\n",
      "Epoch 21/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0815 - val_loss: 0.0900\n",
      "Epoch 22/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0808 - val_loss: 0.0894\n",
      "Epoch 23/60\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0796 - val_loss: 0.0870\n",
      "Epoch 24/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0792 - val_loss: 0.0883\n",
      "Epoch 25/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0779 - val_loss: 0.0872\n",
      "Epoch 26/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0773 - val_loss: 0.0885\n",
      "Epoch 27/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0767 - val_loss: 0.0844\n",
      "Epoch 28/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0762 - val_loss: 0.0872\n",
      "Epoch 29/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0757 - val_loss: 0.0861\n",
      "Epoch 30/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0749 - val_loss: 0.0851\n",
      "Epoch 31/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0745 - val_loss: 0.0819\n",
      "Epoch 32/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0744 - val_loss: 0.0820\n",
      "Epoch 33/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0737 - val_loss: 0.0824\n",
      "Epoch 34/60\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0728 - val_loss: 0.0840\n",
      "Epoch 35/60\n",
      "472432/472432 [==============================] - 22s 48us/step - loss: 0.0724 - val_loss: 0.0809\n",
      "Epoch 36/60\n",
      "472432/472432 [==============================] - 23s 49us/step - loss: 0.0716 - val_loss: 0.0827\n",
      "Epoch 37/60\n",
      "472432/472432 [==============================] - 20s 42us/step - loss: 0.0712 - val_loss: 0.0827\n",
      "Epoch 38/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0712 - val_loss: 0.0820\n",
      "Epoch 39/60\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0708 - val_loss: 0.0822\n",
      "Epoch 40/60\n"
=======
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0758 - val_loss: 0.0917\n",
      "Epoch 21/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0759 - val_loss: 0.0929\n",
      "Epoch 22/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0753 - val_loss: 0.0941\n",
      "Epoch 23/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0751 - val_loss: 0.0991\n",
      "Epoch 24/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0749 - val_loss: 0.0916\n",
      "Epoch 25/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0748 - val_loss: 0.0904\n",
      "Epoch 26/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0748 - val_loss: 0.0926\n",
      "Epoch 27/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0747 - val_loss: 0.0927\n",
      "Epoch 28/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0742 - val_loss: 0.0947\n",
      "Epoch 29/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0741 - val_loss: 0.0910\n",
      "Epoch 30/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0741 - val_loss: 0.0921\n",
      "Epoch 31/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0737 - val_loss: 0.0912\n",
      "Epoch 32/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0739 - val_loss: 0.0901\n",
      "Epoch 33/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0736 - val_loss: 0.0910\n",
      "Epoch 34/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0733 - val_loss: 0.0893\n",
      "Epoch 35/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0734 - val_loss: 0.0938\n",
      "Epoch 36/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0729 - val_loss: 0.0917\n",
      "Epoch 37/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0735 - val_loss: 0.0926\n",
      "Epoch 38/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0730 - val_loss: 0.0907\n",
      "Epoch 39/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0726 - val_loss: 0.0935\n",
      "Epoch 40/100\n",
      "472432/472432 [==============================] - 25s 53us/step - loss: 0.0728 - val_loss: 0.0923\n",
      "Epoch 41/100\n",
      "472432/472432 [==============================] - 31s 66us/step - loss: 0.0724 - val_loss: 0.0938\n",
      "Epoch 42/100\n",
      "472432/472432 [==============================] - 31s 66us/step - loss: 0.0724 - val_loss: 0.0912\n",
      "Epoch 43/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0726 - val_loss: 0.0914\n",
      "Epoch 44/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0721 - val_loss: 0.0924\n",
      "Epoch 45/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0723 - val_loss: 0.0908\n",
      "Epoch 46/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0717 - val_loss: 0.0955\n",
      "Epoch 47/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0723 - val_loss: 0.0921\n",
      "Epoch 48/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0721 - val_loss: 0.0923\n",
      "Epoch 49/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0723 - val_loss: 0.0918\n",
      "Epoch 50/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0720 - val_loss: 0.0918\n",
      "Epoch 51/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0721 - val_loss: 0.0904\n",
      "Epoch 52/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0720 - val_loss: 0.0935\n",
      "Epoch 53/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0715 - val_loss: 0.0888\n",
      "Epoch 54/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0716 - val_loss: 0.0909\n",
      "Epoch 55/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0718 - val_loss: 0.0963\n",
      "Epoch 56/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0715 - val_loss: 0.0909\n",
      "Epoch 57/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0718 - val_loss: 0.0906\n",
      "Epoch 58/100\n",
      "472432/472432 [==============================] - 29s 61us/step - loss: 0.0715 - val_loss: 0.0910\n",
      "Epoch 59/100\n",
      "472432/472432 [==============================] - 31s 65us/step - loss: 0.0719 - val_loss: 0.0925\n",
      "Epoch 60/100\n",
      "472432/472432 [==============================] - 31s 66us/step - loss: 0.0717 - val_loss: 0.0950\n",
      "Epoch 61/100\n",
      "472432/472432 [==============================] - 29s 61us/step - loss: 0.0712 - val_loss: 0.0925\n",
      "Epoch 62/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0714 - val_loss: 0.0925\n",
      "Epoch 63/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0712 - val_loss: 0.0913\n",
      "Epoch 64/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0709 - val_loss: 0.0914\n",
      "Epoch 65/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0710 - val_loss: 0.0919\n",
      "Epoch 66/100\n",
      "472432/472432 [==============================] - 31s 66us/step - loss: 0.0715 - val_loss: 0.0966\n",
      "Epoch 67/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0719 - val_loss: 0.0909\n",
      "Epoch 68/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0706 - val_loss: 0.0898\n",
      "Epoch 69/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0713 - val_loss: 0.0939\n",
      "Epoch 70/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0713 - val_loss: 0.0929\n",
      "Epoch 71/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0714 - val_loss: 0.0906\n",
      "Epoch 72/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0712 - val_loss: 0.0936\n",
      "Epoch 73/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0712 - val_loss: 0.0911\n",
      "Epoch 74/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0709 - val_loss: 0.0936\n",
      "Epoch 75/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0710 - val_loss: 0.0919\n",
      "Epoch 76/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0711 - val_loss: 0.0904\n",
      "Epoch 77/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0711 - val_loss: 0.0921\n",
      "Epoch 78/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0711 - val_loss: 0.0957\n",
      "Epoch 79/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0709 - val_loss: 0.0908\n",
      "Epoch 80/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0709 - val_loss: 0.0908\n",
      "Epoch 81/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0708 - val_loss: 0.0906\n",
      "Epoch 82/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0711 - val_loss: 0.0949\n",
      "Epoch 83/100\n",
      "472432/472432 [==============================] - 31s 65us/step - loss: 0.0709 - val_loss: 0.0911\n",
      "Epoch 84/100\n",
      "472432/472432 [==============================] - 32s 67us/step - loss: 0.0711 - val_loss: 0.0973\n",
      "Epoch 85/100\n",
      "472432/472432 [==============================] - 31s 66us/step - loss: 0.0705 - val_loss: 0.0946\n",
      "Epoch 86/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0711 - val_loss: 0.0945\n",
      "Epoch 87/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0705 - val_loss: 0.0930\n",
      "Epoch 88/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0706 - val_loss: 0.0926\n",
      "Epoch 89/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0711 - val_loss: 0.0925\n",
      "Epoch 90/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0699 - val_loss: 0.0913\n",
      "Epoch 91/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0708 - val_loss: 0.0982\n",
      "Epoch 92/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0707 - val_loss: 0.0947\n",
      "Epoch 93/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0714 - val_loss: 0.0944\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0711 - val_loss: 0.0936\n",
      "Epoch 95/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0708 - val_loss: 0.0941\n",
      "Epoch 96/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0704 - val_loss: 0.0909\n",
      "Epoch 97/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0703 - val_loss: 0.0925\n",
      "Epoch 98/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0704 - val_loss: 0.0964\n",
      "Epoch 99/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0708 - val_loss: 0.0937\n",
      "Epoch 100/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0704 - val_loss: 0.0916\n",
      "118108/118108 [==============================] - 1s 11us/step\n",
      "Fold 2. auc: 0.9266.\n",
      "Fold 4 started at Sat Sep  7 11:50:30 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.9400 - val_loss: 0.4810\n",
      "Epoch 2/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.3219 - val_loss: 0.2394\n",
      "Epoch 3/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1825 - val_loss: 0.1601\n",
      "Epoch 4/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.1301 - val_loss: 0.1226\n",
      "Epoch 5/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1074 - val_loss: 0.1103\n",
      "Epoch 6/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0970 - val_loss: 0.1048\n",
      "Epoch 7/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0910 - val_loss: 0.1028\n",
      "Epoch 8/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0873 - val_loss: 0.0984\n",
      "Epoch 9/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0846 - val_loss: 0.0996\n",
      "Epoch 10/100\n",
      "472432/472432 [==============================] - 25s 52us/step - loss: 0.0829 - val_loss: 0.0975\n",
      "Epoch 11/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0818 - val_loss: 0.0972\n",
      "Epoch 12/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0803 - val_loss: 0.0942\n",
      "Epoch 13/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0805 - val_loss: 0.0935\n",
      "Epoch 14/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0795 - val_loss: 0.0926\n",
      "Epoch 15/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0792 - val_loss: 0.0906\n",
      "Epoch 16/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0782 - val_loss: 0.0942\n",
      "Epoch 17/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0774 - val_loss: 0.0924\n",
      "Epoch 18/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0775 - val_loss: 0.0916\n",
      "Epoch 19/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0767 - val_loss: 0.0911\n",
      "Epoch 20/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0761 - val_loss: 0.0902\n",
      "Epoch 21/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0764 - val_loss: 0.0915\n",
      "Epoch 22/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0757 - val_loss: 0.0930\n",
      "Epoch 23/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0752 - val_loss: 0.0913\n",
      "Epoch 24/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0755 - val_loss: 0.0930\n",
      "Epoch 25/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0747 - val_loss: 0.0897\n",
      "Epoch 26/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0747 - val_loss: 0.0907\n",
      "Epoch 27/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0743 - val_loss: 0.0939\n",
      "Epoch 28/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0749 - val_loss: 0.0940\n",
      "Epoch 29/100\n",
      "472432/472432 [==============================] - 30s 64us/step - loss: 0.0745 - val_loss: 0.0899\n",
      "Epoch 30/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0745 - val_loss: 0.0914\n",
      "Epoch 31/100\n",
      "472432/472432 [==============================] - 30s 64us/step - loss: 0.0744 - val_loss: 0.0918\n",
      "Epoch 32/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0735 - val_loss: 0.0879\n",
      "Epoch 33/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0738 - val_loss: 0.0880\n",
      "Epoch 34/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0738 - val_loss: 0.0888\n",
      "Epoch 35/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0736 - val_loss: 0.0894\n",
      "Epoch 36/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0735 - val_loss: 0.0905\n",
      "Epoch 37/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0737 - val_loss: 0.0882\n",
      "Epoch 38/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0735 - val_loss: 0.0873\n",
      "Epoch 39/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0734 - val_loss: 0.0901\n",
      "Epoch 40/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0725 - val_loss: 0.0891\n",
      "Epoch 41/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0731 - val_loss: 0.0882\n",
      "Epoch 42/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0730 - val_loss: 0.0886\n",
      "Epoch 43/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0728 - val_loss: 0.0889\n",
      "Epoch 44/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0726 - val_loss: 0.0904\n",
      "Epoch 45/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0722 - val_loss: 0.0894\n",
      "Epoch 46/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0731 - val_loss: 0.0886\n",
      "Epoch 47/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0725 - val_loss: 0.0877\n",
      "Epoch 48/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0727 - val_loss: 0.0894\n",
      "Epoch 49/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0728 - val_loss: 0.0886\n",
      "Epoch 50/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0723 - val_loss: 0.0887\n",
      "Epoch 51/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0720 - val_loss: 0.0897\n",
      "Epoch 52/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0723 - val_loss: 0.0891\n",
      "Epoch 53/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0729 - val_loss: 0.0915\n",
      "Epoch 54/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0724 - val_loss: 0.0926\n",
      "Epoch 55/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0723 - val_loss: 0.0954\n",
      "Epoch 56/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0726 - val_loss: 0.0918\n",
      "Epoch 57/100\n",
      "472432/472432 [==============================] - 30s 63us/step - loss: 0.0721 - val_loss: 0.0891\n",
      "Epoch 58/100\n",
      "472432/472432 [==============================] - 30s 64us/step - loss: 0.0723 - val_loss: 0.0903\n",
      "Epoch 59/100\n",
      "472432/472432 [==============================] - 29s 62us/step - loss: 0.0720 - val_loss: 0.0940\n",
      "Epoch 60/100\n",
      "472432/472432 [==============================] - 29s 62us/step - loss: 0.0720 - val_loss: 0.0930\n",
      "Epoch 61/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0722 - val_loss: 0.0902\n",
      "Epoch 62/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0720 - val_loss: 0.0928\n",
      "Epoch 63/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0715 - val_loss: 0.0892\n",
      "Epoch 64/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0720 - val_loss: 0.0877\n",
      "Epoch 65/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0716 - val_loss: 0.0897\n",
      "Epoch 66/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0716 - val_loss: 0.0890\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0713 - val_loss: 0.0920\n",
      "Epoch 68/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0716 - val_loss: 0.0884\n",
      "Epoch 69/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0714 - val_loss: 0.0887\n",
      "Epoch 70/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0715 - val_loss: 0.0916\n",
      "Epoch 71/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0718 - val_loss: 0.0908\n",
      "Epoch 72/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0714 - val_loss: 0.0896\n",
      "Epoch 73/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0717 - val_loss: 0.0894\n",
      "Epoch 74/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0714 - val_loss: 0.0880\n",
      "Epoch 75/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0716 - val_loss: 0.0894\n",
      "Epoch 76/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0713 - val_loss: 0.0903\n",
      "Epoch 77/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0717 - val_loss: 0.0883\n",
      "Epoch 78/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0713 - val_loss: 0.0906\n",
      "Epoch 79/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0714 - val_loss: 0.0881\n",
      "Epoch 80/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0713 - val_loss: 0.0899\n",
      "Epoch 81/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0712 - val_loss: 0.0887\n",
      "Epoch 82/100\n",
      "472432/472432 [==============================] - 24s 50us/step - loss: 0.0713 - val_loss: 0.0898\n",
      "Epoch 83/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0715 - val_loss: 0.0885\n",
      "Epoch 84/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0710 - val_loss: 0.0892\n",
      "Epoch 85/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0715 - val_loss: 0.0895\n",
      "Epoch 86/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0707 - val_loss: 0.0888\n",
      "Epoch 87/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0707 - val_loss: 0.0936\n",
      "Epoch 88/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0711 - val_loss: 0.0905\n",
      "Epoch 89/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0712 - val_loss: 0.0898\n",
      "Epoch 90/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0711 - val_loss: 0.0908\n",
      "Epoch 91/100\n",
      "472432/472432 [==============================] - 27s 56us/step - loss: 0.0709 - val_loss: 0.0925\n",
      "Epoch 92/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0712 - val_loss: 0.0884\n",
      "Epoch 93/100\n",
      "472432/472432 [==============================] - 26s 56us/step - loss: 0.0702 - val_loss: 0.0919\n",
      "Epoch 94/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0710 - val_loss: 0.0881\n",
      "Epoch 95/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0706 - val_loss: 0.0899\n",
      "Epoch 96/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0713 - val_loss: 0.0858\n",
      "Epoch 97/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0706 - val_loss: 0.0925\n",
      "Epoch 98/100\n",
      "472432/472432 [==============================] - 29s 62us/step - loss: 0.0713 - val_loss: 0.0881\n",
      "Epoch 99/100\n",
      "472432/472432 [==============================] - 31s 65us/step - loss: 0.0705 - val_loss: 0.0910\n",
      "Epoch 100/100\n",
      "472432/472432 [==============================] - 31s 65us/step - loss: 0.0708 - val_loss: 0.0879\n",
      "118108/118108 [==============================] - 1s 12us/step\n",
      "Fold 3. auc: 0.9463.\n",
      "Fold 5 started at Sat Sep  7 12:32:41 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/100\n",
      "472432/472432 [==============================] - 32s 67us/step - loss: 0.9464 - val_loss: 0.5016\n",
      "Epoch 2/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.3463 - val_loss: 0.2547\n",
      "Epoch 3/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1956 - val_loss: 0.1725\n",
      "Epoch 4/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1388 - val_loss: 0.1413\n",
      "Epoch 5/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.1138 - val_loss: 0.1144\n",
      "Epoch 6/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.1003 - val_loss: 0.1067\n",
      "Epoch 7/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0929 - val_loss: 0.1308\n",
      "Epoch 8/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0888 - val_loss: 0.1025\n",
      "Epoch 9/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0861 - val_loss: 0.0991\n",
      "Epoch 10/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0841 - val_loss: 0.0976\n",
      "Epoch 11/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0826 - val_loss: 0.0989\n",
      "Epoch 12/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0815 - val_loss: 0.1015\n",
      "Epoch 13/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0803 - val_loss: 0.0987\n",
      "Epoch 14/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0797 - val_loss: 0.0988\n",
      "Epoch 15/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0795 - val_loss: 0.1061\n",
      "Epoch 16/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0778 - val_loss: 0.1010\n",
      "Epoch 17/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0777 - val_loss: 0.1025\n",
      "Epoch 18/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0769 - val_loss: 0.0956\n",
      "Epoch 19/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0771 - val_loss: 0.0986\n",
      "Epoch 20/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0766 - val_loss: 0.0961\n",
      "Epoch 21/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0758 - val_loss: 0.0963\n",
      "Epoch 22/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0759 - val_loss: 0.0965\n",
      "Epoch 23/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0755 - val_loss: 0.1001\n",
      "Epoch 24/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0758 - val_loss: 0.0973\n",
      "Epoch 25/100\n",
      "472432/472432 [==============================] - 21s 44us/step - loss: 0.0753 - val_loss: 0.0964\n",
      "Epoch 26/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0749 - val_loss: 0.1001\n",
      "Epoch 27/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0750 - val_loss: 0.0989\n",
      "Epoch 28/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0749 - val_loss: 0.0965\n",
      "Epoch 29/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0743 - val_loss: 0.0964\n",
      "Epoch 30/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0744 - val_loss: 0.0974\n",
      "Epoch 31/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0743 - val_loss: 0.0981\n",
      "Epoch 32/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0741 - val_loss: 0.0999\n",
      "Epoch 33/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0738 - val_loss: 0.0986\n",
      "Epoch 34/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0739 - val_loss: 0.1004\n",
      "Epoch 35/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0733 - val_loss: 0.0991\n",
      "Epoch 36/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0732 - val_loss: 0.1026\n",
      "Epoch 37/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0731 - val_loss: 0.0993\n",
      "Epoch 38/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0735 - val_loss: 0.0974\n",
      "Epoch 39/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0731 - val_loss: 0.0969\n",
      "Epoch 40/100\n"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "472432/472432 [==============================] - 20s 41us/step - loss: 0.0703 - val_loss: 0.0816\n",
      "Epoch 41/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0698 - val_loss: 0.0804\n",
      "Epoch 42/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0698 - val_loss: 0.0824\n",
      "Epoch 43/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0694 - val_loss: 0.0812\n",
      "Epoch 44/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0691 - val_loss: 0.0814\n",
      "Epoch 45/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0681 - val_loss: 0.0806\n",
      "Epoch 46/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0680 - val_loss: 0.0806\n",
      "Epoch 47/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0674 - val_loss: 0.0795\n",
      "Epoch 48/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0678 - val_loss: 0.0816\n",
      "Epoch 49/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0670 - val_loss: 0.0815\n",
      "Epoch 50/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0672 - val_loss: 0.0795\n",
      "Epoch 51/60\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0664 - val_loss: 0.0804\n",
      "Epoch 52/60\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0666 - val_loss: 0.0791\n",
      "Epoch 53/60\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0661 - val_loss: 0.0798\n",
      "Epoch 54/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0655 - val_loss: 0.0781\n",
      "Epoch 55/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0656 - val_loss: 0.0800\n",
      "Epoch 56/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0648 - val_loss: 0.0784\n",
      "Epoch 57/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0647 - val_loss: 0.0770\n",
      "Epoch 58/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0645 - val_loss: 0.0793\n",
      "Epoch 59/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0643 - val_loss: 0.0786\n",
      "Epoch 60/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0642 - val_loss: 0.0768\n",
      "118108/118108 [==============================] - 1s 7us/step\n",
      "Fold 3. auc: 0.9513.\n",
      "Fold 5 started at Sat Sep  7 00:29:44 2019\n",
      "Train on 472432 samples, validate on 118108 samples\n",
      "Epoch 1/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.6890 - val_loss: 0.6514\n",
      "Epoch 2/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.4773 - val_loss: 0.5314\n",
      "Epoch 3/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.3137 - val_loss: 0.3054\n",
      "Epoch 4/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.2225 - val_loss: 0.2314\n",
      "Epoch 5/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1745 - val_loss: 0.1838\n",
      "Epoch 6/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1477 - val_loss: 0.1534\n",
      "Epoch 7/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.1303 - val_loss: 0.1377\n",
      "Epoch 8/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1201 - val_loss: 0.1258\n",
      "Epoch 9/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1126 - val_loss: 0.1186\n",
      "Epoch 10/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1073 - val_loss: 0.1155\n",
      "Epoch 11/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.1031 - val_loss: 0.1077\n",
      "Epoch 12/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0991 - val_loss: 0.1058\n",
      "Epoch 13/60\n",
      "472432/472432 [==============================] - 19s 39us/step - loss: 0.0961 - val_loss: 0.1015\n",
      "Epoch 14/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0934 - val_loss: 0.0993\n",
      "Epoch 15/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0904 - val_loss: 0.0985\n",
      "Epoch 16/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0889 - val_loss: 0.0965\n",
      "Epoch 17/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0882 - val_loss: 0.0947\n",
      "Epoch 18/60\n",
      "472432/472432 [==============================] - 19s 41us/step - loss: 0.0859 - val_loss: 0.0936\n",
      "Epoch 19/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0847 - val_loss: 0.0909\n",
      "Epoch 20/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0839 - val_loss: 0.0913\n",
      "Epoch 21/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0829 - val_loss: 0.0900\n",
      "Epoch 22/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0819 - val_loss: 0.1001\n",
      "Epoch 23/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0809 - val_loss: 0.0916\n",
      "Epoch 24/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0803 - val_loss: 0.0906\n",
      "Epoch 25/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0791 - val_loss: 0.0868\n",
      "Epoch 26/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0785 - val_loss: 0.0889\n",
      "Epoch 27/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0777 - val_loss: 0.0857\n",
      "Epoch 28/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0770 - val_loss: 0.0857\n",
      "Epoch 29/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0765 - val_loss: 0.0839\n",
      "Epoch 30/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0759 - val_loss: 0.0847\n",
      "Epoch 31/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0752 - val_loss: 0.0886\n",
      "Epoch 32/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0747 - val_loss: 0.0849\n",
      "Epoch 33/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0748 - val_loss: 0.0870\n",
      "Epoch 34/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0739 - val_loss: 0.0823\n",
      "Epoch 35/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0731 - val_loss: 0.0851\n",
      "Epoch 36/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0727 - val_loss: 0.0832\n",
      "Epoch 37/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0725 - val_loss: 0.0854\n",
      "Epoch 38/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0718 - val_loss: 0.0816\n",
      "Epoch 39/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0715 - val_loss: 0.0878\n",
      "Epoch 40/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0707 - val_loss: 0.0885\n",
      "Epoch 41/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0701 - val_loss: 0.0816\n",
      "Epoch 42/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0701 - val_loss: 0.0817\n",
      "Epoch 43/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0699 - val_loss: 0.0814\n",
      "Epoch 44/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0697 - val_loss: 0.0808\n",
      "Epoch 45/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0693 - val_loss: 0.0801\n",
      "Epoch 46/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0689 - val_loss: 0.0807\n",
      "Epoch 47/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0685 - val_loss: 0.0816\n",
      "Epoch 48/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0684 - val_loss: 0.0809\n",
      "Epoch 49/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0679 - val_loss: 0.0810\n",
      "Epoch 50/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0676 - val_loss: 0.0802\n",
      "Epoch 51/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0676 - val_loss: 0.0808\n",
      "Epoch 52/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0670 - val_loss: 0.0815\n",
      "Epoch 53/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0663 - val_loss: 0.0801\n",
      "Epoch 54/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0662 - val_loss: 0.0804\n",
      "Epoch 55/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0665 - val_loss: 0.0798\n",
      "Epoch 56/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0658 - val_loss: 0.0802\n",
      "Epoch 57/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0653 - val_loss: 0.0798\n",
      "Epoch 58/60\n",
      "472432/472432 [==============================] - 18s 38us/step - loss: 0.0650 - val_loss: 0.0800\n",
      "Epoch 59/60\n",
      "472432/472432 [==============================] - 18s 39us/step - loss: 0.0648 - val_loss: 0.0793\n",
      "Epoch 60/60\n",
      "472432/472432 [==============================] - 19s 40us/step - loss: 0.0648 - val_loss: 0.0794\n",
      "118108/118108 [==============================] - 1s 7us/step\n",
      "Fold 4. auc: 0.9298.\n",
      "CV mean score: 0.9384, std: 0.0084.\n"
=======
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0730 - val_loss: 0.0955\n",
      "Epoch 41/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0731 - val_loss: 0.1033\n",
      "Epoch 42/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0730 - val_loss: 0.0974\n",
      "Epoch 43/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0729 - val_loss: 0.0956\n",
      "Epoch 44/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0729 - val_loss: 0.0969\n",
      "Epoch 45/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0732 - val_loss: 0.0978\n",
      "Epoch 46/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0732 - val_loss: 0.0963\n",
      "Epoch 47/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0727 - val_loss: 0.0963\n",
      "Epoch 48/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0721 - val_loss: 0.0966\n",
      "Epoch 49/100\n",
      "472432/472432 [==============================] - 22s 46us/step - loss: 0.0727 - val_loss: 0.0975\n",
      "Epoch 50/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0723 - val_loss: 0.1027\n",
      "Epoch 51/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0726 - val_loss: 0.0962\n",
      "Epoch 52/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0725 - val_loss: 0.0986\n",
      "Epoch 53/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0724 - val_loss: 0.0984\n",
      "Epoch 54/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0722 - val_loss: 0.0992\n",
      "Epoch 55/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0725 - val_loss: 0.0998\n",
      "Epoch 56/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0721 - val_loss: 0.0964\n",
      "Epoch 57/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0722 - val_loss: 0.0956\n",
      "Epoch 58/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0723 - val_loss: 0.0977\n",
      "Epoch 59/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0721 - val_loss: 0.0990\n",
      "Epoch 60/100\n",
      "472432/472432 [==============================] - 27s 57us/step - loss: 0.0725 - val_loss: 0.0988\n",
      "Epoch 61/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0719 - val_loss: 0.0968\n",
      "Epoch 62/100\n",
      "472432/472432 [==============================] - 28s 60us/step - loss: 0.0719 - val_loss: 0.0982\n",
      "Epoch 63/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0719 - val_loss: 0.0970\n",
      "Epoch 64/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0714 - val_loss: 0.0996\n",
      "Epoch 65/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0718 - val_loss: 0.1008\n",
      "Epoch 66/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0717 - val_loss: 0.0973\n",
      "Epoch 67/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0717 - val_loss: 0.0994\n",
      "Epoch 68/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0719 - val_loss: 0.0977\n",
      "Epoch 69/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0717 - val_loss: 0.1037\n",
      "Epoch 70/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0716 - val_loss: 0.1022\n",
      "Epoch 71/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0719 - val_loss: 0.0948\n",
      "Epoch 72/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0714 - val_loss: 0.0976\n",
      "Epoch 73/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0721 - val_loss: 0.0981\n",
      "Epoch 74/100\n",
      "472432/472432 [==============================] - 28s 58us/step - loss: 0.0716 - val_loss: 0.0981\n",
      "Epoch 75/100\n",
      "472432/472432 [==============================] - 28s 59us/step - loss: 0.0718 - val_loss: 0.0985\n",
      "Epoch 76/100\n",
      "472432/472432 [==============================] - 32s 67us/step - loss: 0.0718 - val_loss: 0.0998\n",
      "Epoch 77/100\n",
      "472432/472432 [==============================] - 32s 69us/step - loss: 0.0718 - val_loss: 0.0947\n",
      "Epoch 78/100\n",
      "472432/472432 [==============================] - 32s 68us/step - loss: 0.0711 - val_loss: 0.1052\n",
      "Epoch 79/100\n",
      "472432/472432 [==============================] - 32s 69us/step - loss: 0.0711 - val_loss: 0.0991\n",
      "Epoch 80/100\n",
      "472432/472432 [==============================] - 24s 52us/step - loss: 0.0717 - val_loss: 0.0974\n",
      "Epoch 81/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0714 - val_loss: 0.0971\n",
      "Epoch 82/100\n",
      "472432/472432 [==============================] - 21s 45us/step - loss: 0.0711 - val_loss: 0.1005\n",
      "Epoch 83/100\n",
      "472432/472432 [==============================] - 25s 52us/step - loss: 0.0714 - val_loss: 0.0946\n",
      "Epoch 84/100\n",
      "472432/472432 [==============================] - 27s 58us/step - loss: 0.0710 - val_loss: 0.0943\n",
      "Epoch 85/100\n",
      "472432/472432 [==============================] - 29s 61us/step - loss: 0.0715 - val_loss: 0.0946\n",
      "Epoch 86/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0712 - val_loss: 0.0956\n",
      "Epoch 87/100\n",
      "472432/472432 [==============================] - 21s 43us/step - loss: 0.0712 - val_loss: 0.1005\n",
      "Epoch 88/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0715 - val_loss: 0.1003\n",
      "Epoch 89/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0715 - val_loss: 0.0996\n",
      "Epoch 90/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0715 - val_loss: 0.1001\n",
      "Epoch 91/100\n",
      "472432/472432 [==============================] - 23s 49us/step - loss: 0.0708 - val_loss: 0.0987\n",
      "Epoch 92/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0712 - val_loss: 0.0954\n",
      "Epoch 93/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0709 - val_loss: 0.0972\n",
      "Epoch 94/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0712 - val_loss: 0.1014\n",
      "Epoch 95/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0711 - val_loss: 0.1005\n",
      "Epoch 96/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0710 - val_loss: 0.0962\n",
      "Epoch 97/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0712 - val_loss: 0.0953\n",
      "Epoch 98/100\n",
      "472432/472432 [==============================] - 23s 48us/step - loss: 0.0713 - val_loss: 0.0953\n",
      "Epoch 99/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0712 - val_loss: 0.0949\n",
      "Epoch 100/100\n",
      "472432/472432 [==============================] - 22s 47us/step - loss: 0.0714 - val_loss: 0.0954\n",
      "118108/118108 [==============================] - 1s 11us/step\n",
      "Fold 4. auc: 0.9257.\n",
      "CV mean score: 0.9318, std: 0.0097.\n"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "result_dict_keras = train_model_classification(model=NNModel_maker, \n",
    "                                             X=X,\n",
    "                                             X_test=test,\n",
    "                                             y=y, params=params, folds=folds,\n",
    "                                             model_type=train_options['model_type'], \n",
    "                                             eval_metric=train_options['eval_metric'],\n",
    "                                             plot_feature_importance=True,\n",
    "                                             averaging=train_options['averaging'],\n",
    "                                             groups=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:29.479061Z",
     "start_time": "2019-09-06T21:48:29.120043Z"
=======
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T12:09:11.222750Z",
     "start_time": "2019-09-07T12:09:09.407973Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(f'../../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:32.468093Z",
     "start_time": "2019-09-06T21:48:30.233043Z"
=======
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T12:09:14.028767Z",
     "start_time": "2019-09-07T12:09:12.274751Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "sub['isFraud'] = result_dict_keras['prediction']\n",
    "sub.to_csv(f'{model_folder}/ieee_nn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:33.507076Z",
     "start_time": "2019-09-06T21:48:33.238084Z"
=======
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T12:09:15.226753Z",
     "start_time": "2019-09-07T12:09:15.063755Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 1)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 23,
=======
     "execution_count": 40,
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict_keras['prediction'].shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:34.527086Z",
     "start_time": "2019-09-06T21:48:34.271084Z"
=======
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T12:09:16.466751Z",
     "start_time": "2019-09-07T12:09:16.315755Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506691, 789)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 24,
=======
     "execution_count": 41,
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T21:48:35.565101Z",
     "start_time": "2019-09-06T21:48:35.289097Z"
=======
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T12:09:17.687766Z",
     "start_time": "2019-09-07T12:09:17.510753Z"
>>>>>>> 3fb0c10ea0105a24c345928ec31830a94e25113e
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{model_folder}/results_dict.pkl', 'wb') as f:\n",
    "#     q = json.dumps(result_dict_lgb,indent=2)\n",
    "    pickle.dump(result_dict_keras,f)\n",
    "#     f.write(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "754px",
    "left": "1526px",
    "right": "20px",
    "top": "96px",
    "width": "344px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
